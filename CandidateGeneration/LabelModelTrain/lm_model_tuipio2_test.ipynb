{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "800bb084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "import glob\n",
    "import os\n",
    "from hashlib import new\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import functools\n",
    "from itertools import product\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from snorkel.labeling.model import LabelModel as LMsnorkel\n",
    "from snorkel.labeling.model import MajorityLabelVoter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fd36ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02c99641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2Nested(l, nested_length):\n",
    "    return [l[i:i+nested_length] for i in range(0, len(l), nested_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6655bbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch UMLS ranks\n",
    "\n",
    "sum_lf_p = '/mnt/nas2/results/Results/systematicReview/distant_pico/EBM_PICO_GT/lf_p_summary_tuipio2_train.csv'\n",
    "sum_lf_i = '/mnt/nas2/results/Results/systematicReview/distant_pico/EBM_PICO_GT/lf_i_summary_tuipio2_train.csv'\n",
    "sum_lf_o = '/mnt/nas2/results/Results/systematicReview/distant_pico/EBM_PICO_GT/lf_o_summary_tuipio2_train.csv'\n",
    "\n",
    "\n",
    "def fetchRank(sum_lf_d):\n",
    "    \n",
    "    ranked_umls_coverage = dict()    \n",
    "    umls_coverage_ = dict()\n",
    "    \n",
    "    data=pd.read_csv(sum_lf_d, sep='\\t')\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        if row[0].startswith('UMLS_new_fuzzy_'):\n",
    "            umls_coverage_[row[0]] = row[3]\n",
    "    \n",
    "    umls_coverage_sorted = sorted(umls_coverage_.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for i in umls_coverage_sorted:\n",
    "        k = str(i[0]).split('_lf_')[-1]\n",
    "        ranked_umls_coverage[k] = i[1]\n",
    "\n",
    "    return ranked_umls_coverage\n",
    "\n",
    "ranksorted_p_umls = fetchRank(sum_lf_p)\n",
    "ranksorted_i_umls = fetchRank(sum_lf_i)\n",
    "ranksorted_o_umls = fetchRank(sum_lf_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70abc40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition LF's\n",
    "\n",
    "def partitionLFs(umls_d):\n",
    "    \n",
    "    keys = list(umls_d.keys())\n",
    "\n",
    "    partitioned_lfs = [ ]\n",
    "    \n",
    "    for i in range( 0, len(keys) ):\n",
    "\n",
    "        if i == 0 or i == len(keys):\n",
    "            if i == 0:\n",
    "                partitioned_lfs.append( [keys] )\n",
    "            if i ==len(keys):\n",
    "                temp3 = list2Nested(keys, 1)\n",
    "                partitioned_lfs.append( temp3 )\n",
    "        else:\n",
    "            temp1, temp2 = keys[:i] , keys[i:]\n",
    "            temp3 = list2Nested( keys[:i], 1)\n",
    "            temp3.append( keys[i:] )\n",
    "            partitioned_lfs.append( temp3 )\n",
    "    \n",
    "    return partitioned_lfs\n",
    "\n",
    "\n",
    "partitioned_p_umls = partitionLFs(ranksorted_p_umls)\n",
    "partitioned_i_umls = partitionLFs(ranksorted_i_umls)\n",
    "partitioned_o_umls = partitionLFs(ranksorted_o_umls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0caad81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import LMutils\n",
    "\n",
    "# validation_labels   \n",
    "# validation_labels_tui_pio2   \n",
    "file = '/mnt/nas2/results/Results/systematicReview/distant_pico/EBM_PICO_GT/validation_labels_tui_pio2.tsv'\n",
    "df_data = pd.read_csv(file, sep='\\t', header=0)\n",
    "\n",
    "Y_tokens = df_data['tokens']\n",
    "df_data_train, df_data_val = train_test_split(df_data, test_size=0.20, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3ea54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Candidate labels from multiple LFs\n",
    "indir = '/mnt/nas2/results/Results/systematicReview/distant_pico/candidate_generation'\n",
    "pathlist = Path(indir).glob('**/*.tsv')\n",
    "\n",
    "tokens = []\n",
    "\n",
    "lfs = dict()\n",
    "\n",
    "for file in pathlist:\n",
    "\n",
    "    k = str( file ).split('candidate_generation/')[-1].replace('.tsv', '').replace('/', '_')\n",
    "    mypath = Path(file)\n",
    "    if mypath.stat().st_size != 0:\n",
    "        data = pd.read_csv(file, sep='\\t', header=0)\n",
    "    if len(tokens) == 0:\n",
    "        tokens.extend( list(data.tokens) )\n",
    "    \n",
    "    sab = data.columns[-1]\n",
    "    if len(list( data[sab] )) == 1354953:\n",
    "        lfs[str(k)] = list( data[sab] )[:len(Y_tokens)]\n",
    "\n",
    "\n",
    "print( 'Total number of tokens in validation set: ', len(tokens) )\n",
    "print( 'Total number of LFs in the dictionary', len(lfs) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6931be43",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_test_ebm = '/mnt/nas2/results/Results/systematicReview/distant_pico/EBM_PICO_GT/test_ebm_labels_tui_pio2.tsv'\n",
    "df_data_test_ebm = pd.read_csv(file_test_ebm, sep='\\t', header=0)\n",
    "y_tokens = df_data_test_ebm['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0881d4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in validation set:  51784\n",
      "Total number of LFs in the dictionary 174\n"
     ]
    }
   ],
   "source": [
    "# Read Candidate labels from multiple LFs\n",
    "indir_test_ebm = '/mnt/nas2/results/Results/systematicReview/distant_pico/test_ebm_candidate_generation'\n",
    "pathlist_test_ebm = Path(indir_test_ebm).glob('**/*.tsv')\n",
    "\n",
    "tokens_test_ebm = []\n",
    "\n",
    "lfs_test_ebm = dict()\n",
    "\n",
    "for file in pathlist_test_ebm:\n",
    "    k = str( file ).split('test_ebm_candidate_generation/')[-1].replace('.tsv', '').replace('/', '_')\n",
    "    mypath = Path(file)\n",
    "    \n",
    "    if mypath.stat().st_size != 0:\n",
    "        data_test_ebm = pd.read_csv(file, sep='\\t', header=0)\n",
    "    if len(tokens_test_ebm) == 0:\n",
    "        tokens_test_ebm.extend( list(data_test_ebm.tokens) )\n",
    "        \n",
    "    sab_test_ebm = data_test_ebm.columns[-1]\n",
    "    if len(list( data_test_ebm[sab_test_ebm] )) == 51784:\n",
    "        lfs_test_ebm[str(k)] = list( data_test_ebm[sab_test_ebm] )[:len(y_tokens)]\n",
    "\n",
    "\n",
    "print( 'Total number of tokens in validation set: ', len(tokens_test_ebm) )\n",
    "print( 'Total number of LFs in the dictionary', len(lfs_test_ebm) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bfc944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lf_levels(umls_d:dict, pattern:str, picos:str):\n",
    "\n",
    "    umls_level = dict()\n",
    "\n",
    "    for key, value in umls_d.items():   # iter on both keys and values\n",
    "        search_pattern = pattern + picos\n",
    "        if key.startswith(search_pattern):\n",
    "            k = str(key).split('_lf_')[-1]\n",
    "            umls_level[ k ] = value\n",
    "\n",
    "    return umls_level\n",
    "\n",
    "# Level 1: UMLS\n",
    "umls_p = lf_levels(lfs, 'UMLS_new_fuzzy_', 'p')\n",
    "umls_i = lf_levels(lfs, 'UMLS_new_fuzzy_', 'i')\n",
    "umls_o = lf_levels(lfs, 'UMLS_new_fuzzy_', 'o')\n",
    "umls_p_test_ebm = lf_levels(lfs_test_ebm, 'UMLS_new_fuzzy_', 'p')\n",
    "umls_i_test_ebm = lf_levels(lfs_test_ebm, 'UMLS_new_fuzzy_', 'i')\n",
    "umls_o_test_ebm = lf_levels(lfs_test_ebm, 'UMLS_new_fuzzy_', 'o')\n",
    "\n",
    "# Level 2: non UMLS\n",
    "nonumls_p = lf_levels(lfs, 'nonUMLS_fuzzy_', 'P')\n",
    "nonumls_i = lf_levels(lfs, 'nonUMLS_fuzzy_', 'I')\n",
    "nonumls_o = lf_levels(lfs, 'nonUMLS_fuzzy_', 'O')\n",
    "nonumls_p_test_ebm = lf_levels(lfs_test_ebm, 'nonUMLS_fuzzy_', 'P')\n",
    "nonumls_i_test_ebm = lf_levels(lfs_test_ebm, 'nonUMLS_fuzzy_', 'I')\n",
    "nonumls_o_test_ebm = lf_levels(lfs_test_ebm, 'nonUMLS_fuzzy_', 'O')\n",
    "\n",
    "# Level 3: DS\n",
    "ds_p = lf_levels(lfs, 'DS_fuzzy_', 'P')\n",
    "ds_i = lf_levels(lfs, 'DS_fuzzy_', 'I')\n",
    "ds_o = lf_levels(lfs, 'DS_fuzzy_', 'O')\n",
    "ds_p_test_ebm = lf_levels(lfs_test_ebm, 'DS_fuzzy_', 'P')\n",
    "ds_i_test_ebm = lf_levels(lfs_test_ebm, 'DS_fuzzy_', 'I')\n",
    "ds_o_test_ebm = lf_levels(lfs_test_ebm, 'DS_fuzzy_', 'O')\n",
    "\n",
    "# Level 4: dictionary, rules, heuristics\n",
    "heur_p = lf_levels(lfs, 'heuristics_direct_', 'P')\n",
    "heur_i = lf_levels(lfs, 'heuristics_direct_', 'I')\n",
    "heur_o = lf_levels(lfs, 'heuristics_direct_', 'O')\n",
    "heur_p_test_ebm = lf_levels(lfs_test_ebm, 'heuristics_direct_', 'P')\n",
    "heur_i_test_ebm = lf_levels(lfs_test_ebm, 'heuristics_direct_', 'I')\n",
    "heur_o_test_ebm = lf_levels(lfs_test_ebm, 'heuristics_direct_', 'O')\n",
    "\n",
    "dict_p = lf_levels(lfs, 'dictionary_direct_', 'P')\n",
    "dict_i = lf_levels(lfs, 'dictionary_direct_', 'I')\n",
    "dict_o = lf_levels(lfs, 'dictionary_direct_', 'O')\n",
    "dict_p_test_ebm = lf_levels(lfs_test_ebm, 'dictionary_direct_', 'P')\n",
    "dict_i_test_ebm = lf_levels(lfs_test_ebm, 'dictionary_direct_', 'I')\n",
    "dict_o_test_ebm = lf_levels(lfs_test_ebm, 'dictionary_direct_', 'O')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4027dccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_param_grid(param_grid, seed):\n",
    "    \"\"\" Sample parameter grid\n",
    "\n",
    "    :param param_grid:\n",
    "    :param seed:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    rstate = np.random.get_state()\n",
    "    np.random.seed(seed)\n",
    "    params = list(product(*[param_grid[name] for name in param_grid]))\n",
    "    np.random.shuffle(params)\n",
    "    np.random.set_state(rstate)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c848998f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(s, t):\n",
    "    return sorted(s) == sorted(t)\n",
    "\n",
    "def getLFs(partition:list, umls_d:dict, seed_len:int):\n",
    "\n",
    "    all_lfs_combined = []\n",
    "    \n",
    "    for lf in partition: # for each lf in a partition\n",
    "        \n",
    "        combine_here = [0] * seed_len\n",
    "\n",
    "        for sab in lf:\n",
    "            new_a = umls_d[sab]\n",
    "            old_a = combine_here\n",
    "            temp_a = []\n",
    "            for o_a, n_a in zip(old_a, new_a):\n",
    "                if compare([o_a, n_a] ,[-1, 1]) == True:\n",
    "                    replace_a = max( o_a, n_a )\n",
    "                    temp_a.append( replace_a )\n",
    "                elif compare([o_a, n_a] ,[0, 1]) == True:\n",
    "                    replace_a = max( o_a, n_a )\n",
    "                    temp_a.append( replace_a )\n",
    "                elif compare([o_a, n_a] ,[-1, 0]) == True:\n",
    "                    replace_a = min( o_a, n_a )\n",
    "                    temp_a.append( replace_a )\n",
    "                else:\n",
    "                    temp_a.append( o_a )\n",
    "\n",
    "            combine_here = temp_a\n",
    "\n",
    "        all_lfs_combined.append( combine_here )\n",
    "\n",
    "    return all_lfs_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f378e1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(model_class,\n",
    "                model_class_init,\n",
    "                param_grid,\n",
    "                train=None,\n",
    "                dev=None,\n",
    "                other_train=None,\n",
    "                n_model_search=5,\n",
    "                val_metric='f1_macro',\n",
    "                seed=1234,\n",
    "                checkpoint_gt_mv=False,\n",
    "                tag_fmt_ckpnt='IO'):\n",
    "    \n",
    "    \n",
    "    \"\"\"Simple grid search helper function\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_class\n",
    "    model_class_init\n",
    "    param_grid\n",
    "    train\n",
    "    dev\n",
    "    n_model_search\n",
    "    val_metric\n",
    "    seed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    L_train, Y_train = train\n",
    "    L_dev, Y_dev = dev\n",
    "    \n",
    "    # sample configs\n",
    "    params = sample_param_grid(param_grid, seed)[:n_model_search]\n",
    "    \n",
    "    defaults = {'seed': seed}\n",
    "    best_score, best_config = 0.0, None\n",
    "    print(f\"Grid search over {len(params)} configs\")\n",
    "    \n",
    "    for i, config in enumerate(params):\n",
    "        print(f'[{i}] Label Model')\n",
    "        config = dict(zip(param_grid.keys(), config))\n",
    "        # update default params if not specified\n",
    "        config.update({param: value for param, value in defaults.items() if param not in config})\n",
    "\n",
    "        model = model_class(**model_class_init)\n",
    "        model.fit(L_train, Y_dev, **config)\n",
    "        \n",
    "        y_pred = model.predict(L_dev)\n",
    "        \n",
    "        # set gold tags for evaluation\n",
    "        if tag_fmt_ckpnt == 'IO':\n",
    "            y_gold = np.array([0 if y == 0 else 1 for y in Y_dev])\n",
    "        else:\n",
    "            y_gold = Y_dev\n",
    "            \n",
    "            \n",
    "        if -1 in y_pred:\n",
    "            print(\"Label model predicted -1 (TODO: this happens inconsistently)\")\n",
    "            continue\n",
    "            \n",
    "        # use internal label model scorer to score the prediction\n",
    "        metrics = model.score(L=L_dev,\n",
    "                              Y=y_gold,\n",
    "                              metrics=['accuracy', 'precision', 'recall', 'f1', 'f1_macro'],\n",
    "                              tie_break_policy='random')\n",
    "        \n",
    "    \n",
    "        # compare learned model against MV on same labeled dev set\n",
    "        # skip if LM less than MV\n",
    "        if checkpoint_gt_mv:\n",
    "            mv_metrics = model.score(L=L_dev,\n",
    "                                  Y=y_gold,\n",
    "                                  metrics=['accuracy', 'precision', 'recall', 'f1', 'f1_macro'],\n",
    "                                  tie_break_policy='random')\n",
    "\n",
    "            if metrics[val_metric] < mv_metrics[val_metric]:\n",
    "                continue\n",
    "                \n",
    "        if not best_score or metrics[val_metric] > best_score[val_metric]:\n",
    "            print(config)\n",
    "            best_score = metrics\n",
    "            best_config = config\n",
    "            \n",
    "            # print training set score if we have labeled data\n",
    "            if np.any(Y_train):\n",
    "                y_pred = model.predict(L_train)\n",
    "\n",
    "                if tag_fmt_ckpnt == 'IO':\n",
    "                    y_gold = np.array([0 if y == 0 else 1 for y in Y_train])\n",
    "                else:\n",
    "                    y_gold = Y_train\n",
    "\n",
    "                metrics = model.score(L=L_train,\n",
    "                                      Y=y_gold,\n",
    "                                      metrics=['accuracy', 'precision', 'recall', 'f1', 'f1_macro'],\n",
    "                                      tie_break_policy='random')\n",
    "\n",
    "                print('[TRAIN] {}'.format(' | '.join([f'{m}: {v * 100:2.2f}' for m, v in metrics.items()])))\n",
    "\n",
    "            print('[DEV]   {}'.format(' | '.join([f'{m}: {v * 100:2.2f}' for m, v in best_score.items()])))\n",
    "            print('-' * 88)\n",
    "            \n",
    "            \n",
    "    # retrain best model\n",
    "    print('BEST')\n",
    "    print(best_config)\n",
    "    model = model_class(**model_class_init)\n",
    "    \n",
    "    model.fit(L_train, Y_dev, **best_config)\n",
    "    return model, best_config, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f45d67d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(partitioned_d_umls, train_lfs, test_lfs, df_data_train, df_data_val, df_data_test, picos, paramgrid):\n",
    "\n",
    "    umls_d, non_umls_d, ds_d, heur_d, dict_d = train_lfs\n",
    "    umls_d_test, non_umls_d_test, ds_d_test, heur_d_test, dict_d_test = test_lfs\n",
    "\n",
    "    best_f1_macro = 0.0\n",
    "    best_overall_model = ''\n",
    "    best_overall_config = ''  \n",
    "    \n",
    "    model_class_init = {\n",
    "        'cardinality': 2, \n",
    "        'verbose': True\n",
    "    }\n",
    "\n",
    "    num_hyperparams = functools.reduce(lambda x,y:x*y, [len(x) for x in param_grid.values()])\n",
    "    print(\"Hyperparamater Search Space:\", num_hyperparams)\n",
    "    n_model_search = 25\n",
    "    \n",
    "    \n",
    "    '''#########################################################################\n",
    "    # Choosing the number of LF's from UMLS all\n",
    "    #########################################################################'''\n",
    "            \n",
    "    for i, partition in enumerate(partitioned_d_umls):\n",
    "\n",
    "        combined_lf = getLFs(partition, umls_d, len(Y_tokens))\n",
    "        assert len(partition) == len(combined_lf)\n",
    "\n",
    "        print( 'Total number of UMLS partitions: ', len(partition) )\n",
    "        combined_lf.extend( list(non_umls_d.values()) ) # Combine with level 2\n",
    "        combined_lf.extend( list(ds_d.values()) ) # Combine with level 3\n",
    "        combined_lf.extend( list(heur_d.values()) ) # Combine with level 4\n",
    "        combined_lf.extend( list(dict_d.values()) ) # combine with level 4\n",
    "\n",
    "        L = np.array(combined_lf)\n",
    "        L = np.transpose(L)\n",
    "        L_train, L_val = train_test_split(L, test_size=0.20, shuffle=False)\n",
    "\n",
    "        Y_train = df_data_train[picos]\n",
    "        Y_val = df_data_val[picos]\n",
    "        Y_test = df_data_test[picos]\n",
    "        \n",
    "        best_model, best_config, best_score = grid_search(LMsnorkel, \n",
    "                                           model_class_init, \n",
    "                                           paramgrid,\n",
    "                                           train = (L_train, Y_train),\n",
    "                                           dev = (L_val, Y_val),\n",
    "                                           n_model_search=n_model_search, \n",
    "                                           val_metric='f1_macro', \n",
    "                                           seed=1234,\n",
    "                                           tag_fmt_ckpnt='IO')\n",
    "        \n",
    "        \n",
    "        # Use the best model to predict on the test set\n",
    "        combined_lf_test = getLFs(partition, umls_d_test, len(y_tokens))\n",
    "        assert len(partition) == len(combined_lf_test)\n",
    "        \n",
    "        print( 'Total number of UMLS partitions: ', len(partition) )\n",
    "        combined_lf_test.extend( list(non_umls_d_test.values()) ) # Combine with level 2\n",
    "        combined_lf_test.extend( list(ds_d_test.values()) ) # Combine with level 3\n",
    "        combined_lf_test.extend( list(heur_d_test.values()) ) # Combine with level 4\n",
    "        combined_lf_test.extend( list(dict_d_test.values()) ) # combine with level 4\n",
    "\n",
    "        L_test = np.array(combined_lf_test)\n",
    "        L_test = np.transpose(L_test)\n",
    "        y_pred_test = best_model.predict(L_test)\n",
    "        test_metrics = best_model.score(L=L_test,\n",
    "                              Y=Y_test,\n",
    "                              metrics=['accuracy', 'precision', 'recall', 'f1', 'f1_macro'],\n",
    "                              tie_break_policy='random')\n",
    "        print('[TEST]   {}'.format(' | '.join([f'{m}: {v * 100:2.2f}' for m, v in test_metrics.items()])))\n",
    "        \n",
    "        \n",
    "        if test_metrics['f1_macro'] > best_f1_macro:\n",
    "            best_f1_macro = test_metrics['f1_macro']\n",
    "            best_overall_model = best_model\n",
    "            best_overall_config = best_config\n",
    "            \n",
    "        \n",
    "        print('Best overall macro F1 score: ', best_f1_macro)\n",
    "        print('Best overall configuration: ', best_overall_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "72b932a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'lr': [0.001, 0.0001],\n",
    "    'l2': [0.001, 0.0001],\n",
    "    'n_epochs': [50, 100, 200, 600, 700, 1000, 2000],\n",
    "    'prec_init': [0.6, 0.7, 0.8, 0.9],\n",
    "    'optimizer': [\"adamax\", \"adam\", \"sgd\"],\n",
    "    'lr_scheduler': ['constant'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8a314d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lfs = umls_p, nonumls_p, ds_p, heur_p, dict_p\n",
    "test_lfs = umls_p_test_ebm, nonumls_p_test_ebm, ds_p_test_ebm, heur_p_test_ebm, dict_p_test_ebm\n",
    "\n",
    "train(partitioned_p_umls, train_lfs, test_lfs, df_data_train, df_data_val, df_data_test_ebm, 'p', paramgrid = param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827644f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
