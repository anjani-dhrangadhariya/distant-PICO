{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b3b4e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "import glob\n",
    "import os\n",
    "from hashlib import new\n",
    "from pathlib import Path\n",
    "import time\n",
    "from itertools import product\n",
    "\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from snorkel.labeling.model import LabelModel as LMsnorkel\n",
    "from snorkel.labeling.model import MajorityLabelVoter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0cd54fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7798c0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5d1d2219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2Nested(l, nested_length):\n",
    "    return [l[i:i+nested_length] for i in range(0, len(l), nested_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "daaad8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelModel_mapper_LF = {1:1, -1:0, 0:-1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0638a0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import LMutils\n",
    "\n",
    "train_file = '/mnt/nas2/results/Results/systematicReview/distant_pico/EBM_PICO_GT/v4/gt/train_ebm_labels_tui_pio3.tsv'\n",
    "training_data = pd.read_csv(train_file, sep='\\t', header=0)\n",
    "\n",
    "ebm_test_file = '/mnt/nas2/results/Results/systematicReview/distant_pico/EBM_PICO_GT/v4/gt/test_ebm_labels_tui_pio3.tsv'\n",
    "test_ebm_data = pd.read_csv(ebm_test_file, sep='\\t', header=0)\n",
    "test_ebm_data.rename( columns={'Unnamed: 0':'series'}, inplace=True )\n",
    "\n",
    "physio_test_file = '/mnt/nas2/results/Results/systematicReview/distant_pico/EBM_PICO_GT/v4/gt/test_physio_labels_tui_pio3.tsv'\n",
    "test_physio_data = pd.read_csv(physio_test_file, sep='\\t', header=0)\n",
    "test_physio_data.rename( columns={'Unnamed: 0':'series'}, inplace=True )\n",
    "\n",
    "ebm_test_corrected_file = '/mnt/nas2/results/Results/systematicReview/distant_pico/EBM_PICO_GT/v4/gt/test_ebm_correctedlabels_tui_pio3.tsv'\n",
    "test_physio_corrected_data = pd.read_csv(ebm_test_corrected_file, sep='\\t', header=0)\n",
    "test_physio_corrected_data.rename( columns={'Unnamed: 0':'series'}, inplace=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fff28521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_df(df):\n",
    "\n",
    "    df_series = [ index for index, value in df.tokens.items() for word in ast.literal_eval(value) ]\n",
    "    df_tokens = [ word for index, value in df.tokens.items() for word in ast.literal_eval(value) ]\n",
    "    df_pos = [ word for index, value in df.pos.items() for word in ast.literal_eval(value) ]\n",
    "    df_offsets = [ word for index, value in df.offsets.items() for word in ast.literal_eval(value) ]\n",
    "\n",
    "\n",
    "    df_p = [ int(lab) for index, value in df.p.items() for lab in ast.literal_eval(value) ]\n",
    "    df_p_fine = [ int(lab) for index, value in df.p_f.items() for lab in ast.literal_eval(value) ]\n",
    "    df_i = [ int(lab) for index, value in df.i.items() for lab in ast.literal_eval(value) ]\n",
    "    df_i_fine = [ int(lab) for index, value in df.i_f.items() for lab in ast.literal_eval(value) ]\n",
    "    df_o = [ int(lab) for index, value in df.o.items() for lab in ast.literal_eval(value) ]\n",
    "    df_o_fine = [ int(lab) for index, value in df.o_f.items() for lab in ast.literal_eval(value) ]\n",
    "    \n",
    "    df_flattened = pd.DataFrame({ 'series': df_series,\n",
    "                        'tokens' : df_tokens,\n",
    "                        'offsets': df_offsets,\n",
    "                        'pos': df_pos,\n",
    "                        'p' : df_p,\n",
    "                        'i' : df_i,\n",
    "                        'o' : df_o,\n",
    "                        'p_f' : df_p_fine,\n",
    "                        'i_f' : df_i_fine,\n",
    "                        'o_f' : df_o_fine })\n",
    "    \n",
    "    return df_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b8e07f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the dataframes (currently only the training dataframe and test ebm dataframe with corrected labels can be flattened)\n",
    "data_df = flatten_df(training_data)\n",
    "test_ebm_data = flatten_df(test_ebm_data)\n",
    "test_ebm_corr_df = flatten_df(test_ebm_corrected_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0c041c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = [\n",
    "    data_df.series.to_numpy() ,\n",
    "    test_ebm_data.series.to_numpy() ,\n",
    "    test_physio_data.series.to_numpy(),   \n",
    "    test_ebm_corr_df.series.to_numpy()\n",
    "]\n",
    "\n",
    "\n",
    "sents = [\n",
    "    data_df.tokens.to_numpy() ,\n",
    "    test_ebm_data.tokens.to_numpy() ,\n",
    "    test_physio_data.tokens.to_numpy(),   \n",
    "    test_ebm_corr_df.tokens.to_numpy()    \n",
    "]\n",
    "\n",
    "\n",
    "part_of_speech = [\n",
    "    data_df.pos.to_numpy() ,\n",
    "    test_ebm_data.pos.to_numpy() ,\n",
    "    test_physio_data.pos.to_numpy(),   \n",
    "    test_ebm_corr_df.pos.to_numpy()     \n",
    "]\n",
    "\n",
    "\n",
    "offsets = [\n",
    "    data_df.offsets.to_numpy() ,\n",
    "    test_ebm_data.offsets.to_numpy() ,\n",
    "    test_physio_data.offsets.to_numpy(),   \n",
    "    test_ebm_corr_df.offsets.to_numpy() \n",
    "]\n",
    "\n",
    "\n",
    "Y_p = [\n",
    "    data_df.p.to_numpy() , # 0 -7\n",
    "    data_df.p_f.to_numpy() , # 1 -6\n",
    "    test_ebm_data.p.to_numpy() , # 2 -5\n",
    "    test_ebm_data.p_f.to_numpy() , # 3 -4\n",
    "    test_physio_data.p.to_numpy(),  # 4 -3\n",
    "    test_ebm_corr_df.p.to_numpy(),   # 5 -2\n",
    "    test_ebm_corr_df.p_f.to_numpy() # 6 -1\n",
    "]\n",
    "\n",
    "\n",
    "Y_i = [\n",
    "    data_df.i.to_numpy() ,\n",
    "    data_df.i_f.to_numpy() ,\n",
    "    test_ebm_data.i.to_numpy() ,\n",
    "    test_physio_data.i.to_numpy() \n",
    "]\n",
    "\n",
    "\n",
    "Y_o = [\n",
    "    data_df.o.to_numpy() ,\n",
    "    data_df.o_f.to_numpy() ,\n",
    "    test_ebm_data.o.to_numpy() ,\n",
    "    test_physio_data.o.to_numpy() \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd0f16cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data for error analysis\n",
    "\n",
    "error_analysis_ebm_p = pd.DataFrame({'tokens' : test_ebm_data.tokens,\n",
    "                                'participant' : test_ebm_data.p,\n",
    "                                'participant_fine' : test_ebm_data.p_f }, \n",
    "                                columns=['tokens','participant', 'participant_fine'])\n",
    "\n",
    "#error_analysis_ebm_p.to_csv (r'/mnt/nas2/results/Results/systematicReview/distant_pico/EBM_PICO_GT/error_analysis/test_ebmgold_p', index = None, header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4c4068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data for error analysis\n",
    "\n",
    "error_analysis_ebmcorr_p = pd.DataFrame({'tokens' : test_ebm_corr_df.tokens,\n",
    "                                'participant' : test_ebm_corr_df.p,\n",
    "                                'participant_fine' : test_ebm_corr_df.p_f }, \n",
    "                                columns=['tokens','participant', 'participant_fine'])\n",
    "\n",
    "#error_analysis_ebmcorr_p.to_csv (r'/mnt/nas2/results/Results/systematicReview/distant_pico/EBM_PICO_GT/error_analysis/test_ebmgoldcorr_p', index = None, header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c7fe1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_list(data_column):\n",
    "    return [ word for index, value in data_column.items() for word in ast.literal_eval(value) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "35850d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_array(data_column):\n",
    "    return np.array( [ word for index, value in data_column.items() for word in ast.literal_eval(value) ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9f6e149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_array(label_column):\n",
    "    return np.array( [ labelModel_mapper_LF[int(lab)] for index, value in label_column.items() for k, lab in ast.literal_eval(value).items() ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b873406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lfs(indir):\n",
    "    \n",
    "    pathlist = Path(indir).glob('**/*.tsv')\n",
    "\n",
    "    tokens = ''\n",
    "\n",
    "    lfs = dict()\n",
    "    lfs_lm = dict()\n",
    "\n",
    "    for counter, file in enumerate(pathlist):\n",
    "\n",
    "        k = str( file ).split('/v3/')[-1].replace('.tsv', '').replace('/', '_')\n",
    "        mypath = Path(file)\n",
    "        if mypath.stat().st_size != 0:\n",
    "            data = pd.read_csv(file, sep='\\t', header=0)\n",
    "\n",
    "            data_tokens = data.tokens\n",
    "            if len(tokens) == 0:\n",
    "                tokens = df_to_array(data_tokens)\n",
    "\n",
    "            data_labels = data.labels\n",
    "            #print(len(data_labels))\n",
    "            labels = dict_to_array(data_labels)\n",
    "            #print(len(labels))\n",
    "            if len(labels) != len(tokens):\n",
    "                print(k, len(labels) , len(tokens) )\n",
    "            #assert len(labels) == len(tokens)\n",
    "            lfs[k] = labels\n",
    "\n",
    "\n",
    "    print( 'Total number of tokens in validation set: ', len(tokens) )\n",
    "    print( 'Total number of LFs in the dictionary', len(lfs) )\n",
    "    \n",
    "    return lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1119c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in validation set:  1303169\n",
      "Total number of LFs in the dictionary 842\n"
     ]
    }
   ],
   "source": [
    "indir = '/mnt/nas2/results/Results/systematicReview/distant_pico/training_ebm_candidate_generation/v3'\n",
    "train_ebm_lfs = get_lfs(indir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ea39eaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in validation set:  51784\n",
      "Total number of LFs in the dictionary 842\n"
     ]
    }
   ],
   "source": [
    "indir_test_ebm = '/mnt/nas2/results/Results/systematicReview/distant_pico/test_ebm_candidate_generation/v3'\n",
    "test_ebm_lfs = get_lfs(indir_test_ebm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "456a19e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in validation set:  52582\n",
      "Total number of LFs in the dictionary 842\n"
     ]
    }
   ],
   "source": [
    "indir_test_ebm_corr = '/mnt/nas2/results/Results/systematicReview/distant_pico/test_ebm_anjani_candidate_generation/v3'\n",
    "test_ebm_corr_lfs = get_lfs(indir_test_ebm_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4392530f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping no positive label LFs\n"
     ]
    }
   ],
   "source": [
    "# Remove the annotations where there are no positive labels\n",
    "print('Dropping no positive label LFs')\n",
    "\n",
    "def drop_nopositive(lfs_d):\n",
    "    \n",
    "    dropped_no_positives = dict()\n",
    "\n",
    "    for k, v in lfs_d.items():\n",
    "\n",
    "        if len(set(v)) < 3:\n",
    "            if 1 in list(set(v)):\n",
    "                dropped_no_positives[k] = v\n",
    "        else:\n",
    "            dropped_no_positives[k] = v\n",
    "            \n",
    "    return dropped_no_positives\n",
    "\n",
    "#lfs_dropped = drop_nopositive(lfs)\n",
    "#print('Number of LFs: ', len(lfs_dropped))\n",
    "#lfs_ebm_corr_dropped = drop_nopositive(test_ebm_corr_lfs)\n",
    "#print('Number of LFs: ', len(lfs_ebm_corr_dropped))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "001d73ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lf_levels(umls_d:dict, pattern:str, picos:str):\n",
    "\n",
    "    umls_level = dict()\n",
    "\n",
    "    for key, value in umls_d.items():   # iter on both keys and values\n",
    "        search_pattern = pattern + picos\n",
    "        if key.startswith(search_pattern):\n",
    "            k = str(key).split('_')[-1]\n",
    "            umls_level[ k ] = value\n",
    "\n",
    "    return umls_level\n",
    "\n",
    "\n",
    "# Level 1: UMLS\n",
    "umls_p = [\n",
    "    lf_levels(train_ebm_lfs, name, 'P') \n",
    "    for i, name in enumerate(['UMLS_direct_', 'UMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "umls_p_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'P') \n",
    "    for i, name in enumerate(['UMLS_direct_', 'UMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "umls_p_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'P') \n",
    "    for i, name in enumerate(['UMLS_direct_', 'UMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "umls_i = [\n",
    "    lf_levels(train_ebm_lfs, name, 'I') \n",
    "    for i, name in enumerate(['UMLS_direct_', 'UMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "umls_i_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'I') \n",
    "    for i, name in enumerate(['UMLS_direct_', 'UMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "umls_i_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'I') \n",
    "    for i, name in enumerate(['UMLS_direct_', 'UMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "umls_o = [\n",
    "    lf_levels(train_ebm_lfs, name, 'O') \n",
    "    for i, name in enumerate(['UMLS_direct_', 'UMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "umls_o_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'O') \n",
    "    for i, name in enumerate(['UMLS_direct_', 'UMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "umls_o_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'O') \n",
    "    for i, name in enumerate(['UMLS_direct_', 'UMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Level 2: non UMLS\n",
    "nonumls_p = [\n",
    "    lf_levels(train_ebm_lfs, name, 'P') \n",
    "    for i, name in enumerate(['nonUMLS_direct_', 'nonUMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "nonumls_p_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'P') \n",
    "    for i, name in enumerate(['nonUMLS_direct_', 'nonUMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "nonumls_p_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'P') \n",
    "    for i, name in enumerate(['nonUMLS_direct_', 'nonUMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "nonumls_i = [\n",
    "    lf_levels(train_ebm_lfs, name, 'I') \n",
    "    for i, name in enumerate(['nonUMLS_direct_', 'nonUMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "nonumls_i_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'I') \n",
    "    for i, name in enumerate(['nonUMLS_direct_', 'nonUMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "nonumls_i_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'I') \n",
    "    for i, name in enumerate(['nonUMLS_direct_', 'nonUMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "nonumls_o = [\n",
    "    lf_levels(train_ebm_lfs, name, 'O') \n",
    "    for i, name in enumerate(['nonUMLS_direct_', 'nonUMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "nonumls_o_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'O') \n",
    "    for i, name in enumerate(['nonUMLS_direct_', 'nonUMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "nonumls_o_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'O') \n",
    "    for i, name in enumerate(['nonUMLS_direct_', 'nonUMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "# Level 3: DS\n",
    "ds_p = [\n",
    "    lf_levels(train_ebm_lfs, name, 'P') \n",
    "    for i, name in enumerate(['ds_direct_', 'ds_fuzzy_'])\n",
    "]\n",
    "\n",
    "ds_p_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'P') \n",
    "    for i, name in enumerate(['ds_direct_', 'ds_fuzzy_'])\n",
    "]\n",
    "\n",
    "ds_p_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'P') \n",
    "    for i, name in enumerate(['ds_direct_', 'ds_fuzzy_'])\n",
    "]\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "ds_i = [\n",
    "    lf_levels(train_ebm_lfs, name, 'I') \n",
    "    for i, name in enumerate(['ds_direct_', 'ds_fuzzy_'])\n",
    "]\n",
    "\n",
    "ds_i_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'I') \n",
    "    for i, name in enumerate(['ds_direct_', 'ds_fuzzy_'])\n",
    "]\n",
    "\n",
    "ds_i_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'I') \n",
    "    for i, name in enumerate(['ds_direct_', 'ds_fuzzy_'])\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "ds_o = [\n",
    "    lf_levels(train_ebm_lfs, name, 'O') \n",
    "    for i, name in enumerate(['ds_direct_', 'ds_fuzzy_'])\n",
    "]\n",
    "\n",
    "ds_o_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'O') \n",
    "    for i, name in enumerate(['ds_direct_', 'ds_fuzzy_'])\n",
    "]\n",
    "\n",
    "ds_o_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'O') \n",
    "    for i, name in enumerate(['ds_direct_', 'ds_fuzzy_'])\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Level 4: dictionary, rules, heuristics\n",
    "heur_p = [\n",
    "    lf_levels(train_ebm_lfs, name, 'P') \n",
    "    for i, name in enumerate(['heuristics_direct_'])\n",
    "]\n",
    "\n",
    "heur_p_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'P') \n",
    "    for i, name in enumerate(['heuristics_direct_'])\n",
    "]\n",
    "\n",
    "heur_p_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'P') \n",
    "    for i, name in enumerate(['heuristics_direct_'])\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "heur_i = [\n",
    "    lf_levels(train_ebm_lfs, name, 'I') \n",
    "    for i, name in enumerate(['heuristics_direct_'])\n",
    "]\n",
    "\n",
    "heur_i_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'I') \n",
    "    for i, name in enumerate(['heuristics_direct_'])\n",
    "]\n",
    "\n",
    "heur_i_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'I') \n",
    "    for i, name in enumerate(['heuristics_direct_'])\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "heur_o = [\n",
    "    lf_levels(train_ebm_lfs, name, 'O') \n",
    "    for i, name in enumerate(['heuristics_direct_'])\n",
    "]\n",
    "\n",
    "heur_o_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'O') \n",
    "    for i, name in enumerate(['heuristics_direct_'])\n",
    "]\n",
    "\n",
    "\n",
    "heur_o_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'O') \n",
    "    for i, name in enumerate(['heuristics_direct_'])\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "dict_p = [\n",
    "    lf_levels(train_ebm_lfs, name, 'P') \n",
    "    for i, name in enumerate(['dictionary_direct_', 'dictionary_fuzzy_'])\n",
    "]\n",
    "\n",
    "dict_p_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'P') \n",
    "    for i, name in enumerate(['dictionary_direct_', 'dictionary_fuzzy_'])\n",
    "]\n",
    "\n",
    "dict_p_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'P') \n",
    "    for i, name in enumerate(['dictionary_direct_', 'dictionary_fuzzy_'])\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "dict_i = [\n",
    "    lf_levels(train_ebm_lfs, name, 'I') \n",
    "    for i, name in enumerate(['dictionary_direct_', 'dictionary_fuzzy_'])\n",
    "]\n",
    "\n",
    "dict_i_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'I') \n",
    "    for i, name in enumerate(['dictionary_direct_', 'dictionary_fuzzy_'])\n",
    "]\n",
    "\n",
    "dict_i_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'I') \n",
    "    for i, name in enumerate(['dictionary_direct_', 'dictionary_fuzzy_'])\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "dict_o = [\n",
    "    lf_levels(train_ebm_lfs, name, 'O') \n",
    "    for i, name in enumerate(['dictionary_direct_', 'dictionary_fuzzy_'])\n",
    "]\n",
    "\n",
    "dict_o_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'O') \n",
    "    for i, name in enumerate(['dictionary_direct_', 'dictionary_fuzzy_'])\n",
    "]\n",
    "\n",
    "dict_o_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'O') \n",
    "    for i, name in enumerate(['dictionary_direct_', 'dictionary_fuzzy_'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1435d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_candidates = [umls_p[1], nonumls_p[1], ds_p[1], dict_p[1], heur_p[0]]\n",
    "test_ebm_corr_candidates = [umls_p_testcorrected[1], nonumls_p_testcorrected[1], ds_p_testcorrected[1], dict_p_testcorrected[1], heur_p_testcorrected[0]]\n",
    "test_ebm_candidates = [umls_p_testebm[1], nonumls_p_testebm[1], ds_p_testebm[1], dict_p_testebm[1], heur_p_testebm[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "02cff49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLen(lf):\n",
    "    for l in lf:\n",
    "        for k,v in l.items():\n",
    "            print(len(v))\n",
    "            \n",
    "#getLen(umls_i) \n",
    "#getLen(nonumls_i) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6dff19b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch UMLS ranks\n",
    "\n",
    "sum_lf_p = '/mnt/nas2/results/Results/systematicReview/distant_pico/EBM_PICO_GT/v3/summaries/lf_p_summary_tuipio3_train.csv'\n",
    "sum_lf_i = '/mnt/nas2/results/Results/systematicReview/distant_pico/EBM_PICO_GT/v3/summaries/lf_i_summary_tuipio3_train.csv'\n",
    "sum_lf_o = '/mnt/nas2/results/Results/systematicReview/distant_pico/EBM_PICO_GT/v3/summaries/lf_o_summary_tuipio3_train.csv'\n",
    "\n",
    "\n",
    "def fetchRank(sum_lf_d, pattern, picos: str, drop_nopos: bool):\n",
    "    \n",
    "    drop_nopos_keys = []\n",
    "    for k,v in train_ebm_lfs.items():\n",
    "        query = '_'+picos+'_'\n",
    "        if query in str(k):\n",
    "            key = str(k).split('_lf_')[-1]\n",
    "            drop_nopos_keys.append(key)\n",
    "    \n",
    "    ranked_umls_coverage = dict()    \n",
    "    umls_coverage_ = dict()\n",
    "    \n",
    "    data=pd.read_csv(sum_lf_d, sep='\\t')\n",
    "    \n",
    "    for index, row in data.iterrows():\n",
    "        if row[0].startswith(pattern):\n",
    "            umls_coverage_[row[0]] = row[3]\n",
    "    \n",
    "    umls_coverage_sorted = sorted(umls_coverage_.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for i in umls_coverage_sorted:\n",
    "        k = str(i[0]).split('_')[-1]\n",
    "        if drop_nopos == False:\n",
    "            ranked_umls_coverage[k] = i[1]\n",
    "        else:\n",
    "            if k in drop_nopos_keys:\n",
    "                ranked_umls_coverage[k] = i[1]\n",
    "\n",
    "    return ranked_umls_coverage\n",
    "\n",
    "\n",
    "# fuzzy UMLS\n",
    "ranksorted_p_umls_fuzzy = fetchRank(sum_lf_p, 'UMLS_fuzzy_', picos='P', drop_nopos = False)\n",
    "ranksorted_i_umls_fuzzy = fetchRank(sum_lf_i, 'UMLS_fuzzy_', picos='I', drop_nopos = False)\n",
    "ranksorted_o_umls_fuzzy = fetchRank(sum_lf_o, 'UMLS_fuzzy_', picos='O', drop_nopos = False)\n",
    "\n",
    "\n",
    "# direct UMLS\n",
    "ranksorted_p_umls_direct = fetchRank(sum_lf_p, 'UMLS_direct_', picos='P', drop_nopos = False)\n",
    "ranksorted_i_umls_direct = fetchRank(sum_lf_i, 'UMLS_direct_', picos='I', drop_nopos = False)\n",
    "ranksorted_o_umls_direct = fetchRank(sum_lf_o, 'UMLS_direct_', picos='O', drop_nopos = False)\n",
    "\n",
    "proper_coverage_p = '/mnt/nas2/results/Results/systematicReview/distant_pico/coverage_results/participant_UMLS_v3_coverage.json'\n",
    "\n",
    "with open(proper_coverage_p, 'r') as rf:\n",
    "    data = json.load(rf)\n",
    " \n",
    "    for k, v in data.items():\n",
    "        if k in ranksorted_p_umls_fuzzy:\n",
    "            ranksorted_p_umls_fuzzy[k] = data[k]\n",
    "            \n",
    "ranksorted_coverage_p_umls_fuzzy = sorted(ranksorted_p_umls_fuzzy.items(), key=lambda x: x[1], reverse=True)\n",
    "ranksorted_coverage_p_umls_fuzzy = dict(ranksorted_coverage_p_umls_fuzzy)\n",
    "\n",
    "\n",
    "proper_coverage_i = '/mnt/nas2/results/Results/systematicReview/distant_pico/coverage_results/intervention_UMLS_v3_coverage.json'\n",
    "\n",
    "with open(proper_coverage_i, 'r') as rf:\n",
    "    data = json.load(rf)\n",
    " \n",
    "    for k, v in data.items():\n",
    "        if k in ranksorted_i_umls_fuzzy:\n",
    "            ranksorted_i_umls_fuzzy[k] = data[k]\n",
    "            \n",
    "ranksorted_coverage_i_umls_fuzzy = sorted(ranksorted_i_umls_fuzzy.items(), key=lambda x: x[1], reverse=True)\n",
    "ranksorted_coverage_i_umls_fuzzy = dict(ranksorted_coverage_i_umls_fuzzy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f749e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition LF's\n",
    "\n",
    "def partitionLFs(umls_d):\n",
    "    \n",
    "    keys = list(umls_d.keys())\n",
    "\n",
    "    partitioned_lfs = [ ]\n",
    "    \n",
    "    for i in range( 0, len(keys) ):\n",
    "\n",
    "        if i == 0 or i == len(keys):\n",
    "            if i == 0:\n",
    "                partitioned_lfs.append( [keys] )\n",
    "            if i ==len(keys):\n",
    "                temp3 = list2Nested(keys, 1)\n",
    "                partitioned_lfs.append( temp3 )\n",
    "        else:\n",
    "            temp1, temp2 = keys[:i] , keys[i:]\n",
    "            temp3 = list2Nested( keys[:i], 1)\n",
    "            temp3.append( keys[i:] )\n",
    "            partitioned_lfs.append( temp3 )\n",
    "    \n",
    "    return partitioned_lfs\n",
    "\n",
    "\n",
    "partitioned_p_umls_fuzzy = partitionLFs(ranksorted_coverage_p_umls_fuzzy)\n",
    "partitioned_i_umls_fuzzy = partitionLFs(ranksorted_coverage_i_umls_fuzzy)\n",
    "partitioned_o_umls_fuzzy = partitionLFs(ranksorted_o_umls_fuzzy)\n",
    "\n",
    "partitioned_p_umls_direct = partitionLFs(ranksorted_p_umls_direct)\n",
    "partitioned_i_umls_direct = partitionLFs(ranksorted_i_umls_direct)\n",
    "partitioned_o_umls_direct = partitionLFs(ranksorted_o_umls_direct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d04c58c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_level = ['UMLS', 'UMLS_Ontology', 'UMLS_Ontology_Rules']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2c6e5471",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'lr': [0.001, 0.0001],\n",
    "    'l2': [0.001, 0.0001],\n",
    "    'n_epochs': [50, 100, 200, 600, 700, 1000, 2000],\n",
    "    'prec_init': [0.6, 0.7, 0.8, 0.9],\n",
    "    'optimizer': [\"adamax\", \"adam\", \"sgd\"],\n",
    "    'lr_scheduler': ['constant'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3d083955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_param_grid(param_grid, seed):\n",
    "    \"\"\" Sample parameter grid\n",
    "    :param param_grid:\n",
    "    :param seed:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    rstate = np.random.get_state()\n",
    "    np.random.seed(seed)\n",
    "    params = list(product(*[param_grid[name] for name in param_grid]))\n",
    "    np.random.shuffle(params)\n",
    "    np.random.set_state(rstate)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c0cc37ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(s, t):\n",
    "    return sorted(s) == sorted(t)\n",
    "\n",
    "def getLFs(partition:list, umls_d:dict, seed_len:int):\n",
    "\n",
    "    all_lfs_combined = []\n",
    "    \n",
    "    for lf in partition: # for each lf in a partition\n",
    "        \n",
    "        combine_here = [0] * seed_len\n",
    "\n",
    "        for sab in lf:\n",
    "            new_a = list(umls_d[sab])\n",
    "            old_a = combine_here\n",
    "            temp_a = []\n",
    "            \n",
    "            for o_a, n_a in zip(old_a, new_a):\n",
    "                               \n",
    "                if compare([o_a, n_a] ,[-1, 1]) == True:\n",
    "                    replace_a = max( o_a, n_a )\n",
    "                    temp_a.append( replace_a )\n",
    "                    \n",
    "                elif compare([o_a, n_a] ,[0, 1]) == True:\n",
    "                    replace_a = max( o_a, n_a )\n",
    "                    temp_a.append( replace_a )\n",
    "                    \n",
    "                elif compare([o_a, n_a] ,[-1, 0]) == True:\n",
    "                    replace_a = min( o_a, n_a )\n",
    "                    temp_a.append( replace_a )\n",
    "                else:\n",
    "                    temp_a.append( o_a )\n",
    "\n",
    "            combine_here = temp_a\n",
    "\n",
    "        all_lfs_combined.append( combine_here )\n",
    "\n",
    "    return all_lfs_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "602295fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(model_class,\n",
    "                model_class_init,\n",
    "                param_grid,\n",
    "                train=None,\n",
    "                dev=None,\n",
    "                other_train=None,\n",
    "                n_model_search=5,\n",
    "                val_metric='f1_macro',\n",
    "                seed=1234,\n",
    "                checkpoint_gt_mv=False,\n",
    "                tag_fmt_ckpnt='IO'):\n",
    "    \n",
    "    \n",
    "    \"\"\"Simple grid search helper function\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_class\n",
    "    model_class_init\n",
    "    param_grid\n",
    "    train\n",
    "    dev\n",
    "    n_model_search\n",
    "    val_metric\n",
    "    seed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    \n",
    "    L_train, Y_train = train\n",
    "    L_dev, Y_dev = dev\n",
    "\n",
    "    # sample configs\n",
    "    params = sample_param_grid(param_grid, seed)[:n_model_search]\n",
    "\n",
    "    defaults = {'seed': seed}\n",
    "    best_score, best_config = 0.0, None\n",
    "    print(f\"Grid search over {len(params)} configs\")\n",
    "\n",
    "    for i, config in enumerate(params):\n",
    "        print(f'[{i}] Label Model')\n",
    "        config = dict(zip(param_grid.keys(), config))\n",
    "        config.update({param: value for param, value in defaults.items() if param not in config})\n",
    "\n",
    "        model = model_class(**model_class_init)\n",
    "        model.fit(L_train, Y_dev, **config)\n",
    "        \n",
    "        y_pred = model.predict(L_dev)\n",
    "        \n",
    "        if tag_fmt_ckpnt == 'IO':\n",
    "            y_gold = np.array([0 if y == 0 else 1 for y in Y_dev])\n",
    "        else:\n",
    "            y_gold = Y_dev\n",
    "            \n",
    "            \n",
    "        if -1 in y_pred:\n",
    "            print(\"Label model predicted -1 (TODO: this happens inconsistently)\")\n",
    "            continue\n",
    "            \n",
    "        # use internal label model scorer to score the prediction\n",
    "        metrics = model.score(L=L_dev,\n",
    "                              Y=y_gold,\n",
    "                              metrics=['accuracy', 'precision', 'recall', 'f1', 'f1_macro'],\n",
    "                              tie_break_policy='random')\n",
    "        \n",
    "    \n",
    "        # compare learned model against MV on same labeled dev set\n",
    "        # skip if LM less than MV\n",
    "        if checkpoint_gt_mv:\n",
    "            mv_metrics = model.score(L=L_dev,\n",
    "                                  Y=y_gold,\n",
    "                                  metrics=['accuracy', 'precision', 'recall', 'f1', 'f1_macro'],\n",
    "                                  tie_break_policy='random')\n",
    "\n",
    "            if metrics[val_metric] < mv_metrics[val_metric]:\n",
    "                continue\n",
    "                \n",
    "        if not best_score or metrics[val_metric] > best_score[val_metric]:\n",
    "            print(config)\n",
    "            best_score = metrics\n",
    "            best_config = config\n",
    "            \n",
    "            # print training set score if we have labeled data\n",
    "            if np.any(Y_train):\n",
    "                y_pred = model.predict(L_train)\n",
    "\n",
    "                if tag_fmt_ckpnt == 'IO':\n",
    "                    y_gold = np.array([0 if y == 0 else 1 for y in Y_train])\n",
    "                else:\n",
    "                    y_gold = Y_train\n",
    "\n",
    "                metrics = model.score(L=L_train,\n",
    "                                      Y=y_gold,\n",
    "                                      metrics=['accuracy', 'precision', 'recall', 'f1', 'f1_macro'],\n",
    "                                      tie_break_policy='random')\n",
    "\n",
    "                print('[TRAIN] {}'.format(' | '.join([f'{m}: {v * 100:2.2f}' for m, v in metrics.items()])))\n",
    "\n",
    "            print('[DEV]   {}'.format(' | '.join([f'{m}: {v * 100:2.2f}' for m, v in best_score.items()])))\n",
    "            print('-' * 88)\n",
    "            \n",
    "            \n",
    "    # retrain best model\n",
    "    print('BEST')\n",
    "    print(best_config)\n",
    "    model = model_class(**model_class_init)\n",
    "    \n",
    "    \n",
    "    model.fit(L_train, Y_dev, **best_config)\n",
    "    return model, best_config, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "2f754cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(part, cands, best_model, gt_labels, exp_l):\n",
    "    \n",
    "    if exp_l == 'UMLS':\n",
    "    \n",
    "        combined_lf = getLFs( part, cands[0], len(gt_labels) )\n",
    "        assert len(part) == len(combined_lf)\n",
    "        print( 'Total number of UMLS partitions: ', len(part) )\n",
    "        \n",
    "    elif exp_l == 'UMLS_Ontology':\n",
    "    \n",
    "        combined_lf = getLFs( part, cands[0], len(gt_labels) )\n",
    "        assert len(part) == len(combined_lf)\n",
    "        print( 'Total number of UMLS partitions: ', len(part) )\n",
    "        combined_lf.extend( list(cands[1].values()) ) # Combine with level 2\n",
    "        combined_lf.extend( list(cands[2].values()) ) # Combine with level 3\n",
    "        combined_lf.extend( list(cands[3].values()) ) # Combine with level 4\n",
    "        \n",
    "    elif exp_l == 'UMLS_Ontology_Rules':\n",
    "\n",
    "        combined_lf = getLFs( part, cands[0], len(gt_labels) )\n",
    "        assert len(part) == len(combined_lf)\n",
    "        print( 'Total number of UMLS partitions: ', len(part) )\n",
    "        combined_lf.extend( list(cands[1].values()) ) # Combine with level 2\n",
    "        combined_lf.extend( list(cands[2].values()) ) # Combine with level 3\n",
    "        combined_lf.extend( list(cands[3].values()) ) # Combine with level 4\n",
    "        combined_lf.extend( list(cands[4].values()) ) # combine with level 4\n",
    "\n",
    "\n",
    "    L = np.array( combined_lf )\n",
    "    L = np.transpose(L)\n",
    "    \n",
    "    predictions_probablities = best_model.predict_proba(L)\n",
    "    predictions = best_model.predict(L)\n",
    "    groundtruth = np.array(gt_labels) \n",
    "    groundtruth_ = [1 if x != 0 else x for x in gt_labels] # XXX if \"test_ebm_correct\"\n",
    "    groundtruth = np.array(groundtruth_)\n",
    "\n",
    "    cr = classification_report( groundtruth, predictions )\n",
    "    print( cr )\n",
    "    \n",
    "    return predictions_probablities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "78a1e865",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(partitioned_d_umls, train_cands, test_cands, test_corr_cands, Y_d, picos, paramgrid, mode):\n",
    "   \n",
    "\n",
    "\n",
    "    gold_labels = ''\n",
    "    gold_labels_fine = ''\n",
    "    \n",
    "    \n",
    "    model_class_init = {\n",
    "        'cardinality': 2, \n",
    "        'verbose': True\n",
    "    }\n",
    "\n",
    "    num_hyperparams = functools.reduce(lambda x,y:x*y, [len(x) for x in param_grid.values()])\n",
    "    print(\"Hyperparamater Search Space:\", num_hyperparams)\n",
    "    n_model_search = 50\n",
    "    \n",
    "\n",
    "    '''#########################################################################\n",
    "    # Choosing the number of LF's from UMLS all\n",
    "    #########################################################################'''\n",
    "    \n",
    "    for l in exp_level:\n",
    "        print( 'Executing the experiment level: ', l )\n",
    "        \n",
    "        best_f1_macro = 0.0\n",
    "        best_overall_model = ''\n",
    "        best_overall_config = ''\n",
    "        best_overall_partition = 0\n",
    "        overall_L = ''\n",
    "    \n",
    "        for i, partition in enumerate(partitioned_d_umls):\n",
    "        \n",
    "            #if len(partition) == 2:\n",
    "            \n",
    "            if l == 'UMLS' and i < 2:\n",
    "                continue\n",
    "\n",
    "            if l == 'UMLS':\n",
    "\n",
    "                combined_lf = getLFs(partition, train_cands[0], len( data_df.tokens ))\n",
    "                #combined_lf = combined_lf[2:]  # i>2 (as LM should have at least 3 labeling functions)\n",
    "                assert len(partition) == len(combined_lf)\n",
    "                print( 'Total number of UMLS partitions: ', len(partition) )\n",
    "                \n",
    "            if l == 'UMLS_Ontology':\n",
    "\n",
    "                combined_lf = getLFs(partition, train_cands[0], len( data_df.tokens ))\n",
    "                assert len(partition) == len(combined_lf)\n",
    "                print( 'Total number of UMLS partitions: ', len(partition) )\n",
    "                combined_lf.extend( list(train_cands[1].values()) ) # Combine with level 2\n",
    "                combined_lf.extend( list(train_cands[2].values()) ) # Combine with level 3\n",
    "                combined_lf.extend( list(train_cands[3].values()) ) # Combine with level 4\n",
    "            \n",
    "            if l == 'UMLS_Ontology_Rules':\n",
    "\n",
    "                combined_lf = getLFs(partition, train_cands[0], len( data_df.tokens ))\n",
    "                assert len(partition) == len(combined_lf)\n",
    "                print( 'Total number of UMLS partitions: ', len(partition) )\n",
    "                combined_lf.extend( list(train_cands[1].values()) ) # Combine with level 2\n",
    "                combined_lf.extend( list(train_cands[2].values()) ) # Combine with level 3\n",
    "                combined_lf.extend( list(train_cands[3].values()) ) # Combine with level 4\n",
    "                combined_lf.extend( list(train_cands[4].values()) ) # combine with level 4\n",
    "\n",
    "\n",
    "            L = np.array( combined_lf )\n",
    "            L = np.transpose(L)\n",
    "            L_train, L_val = train_test_split(L, test_size=0.20, shuffle=False)\n",
    "            Y_train, Y_val = train_test_split( np.array(Y_d[0]), test_size=0.20, shuffle=False)\n",
    "            Y_train_fine, Y_val_fine = train_test_split( np.array(Y_d[1]), test_size=0.20, shuffle=False)\n",
    "\n",
    "            # convert the fine labels to 0 and 1\n",
    "            Y_train_fine = [1 if x != 0 else x for x in Y_train_fine]\n",
    "            Y_val_fine = [1 if x != 0 else x for x in Y_val_fine]\n",
    "\n",
    "            #print( Y_val_fine )\n",
    "\n",
    "\n",
    "            #Y_train = Y_d[0]\n",
    "            #Y_val = Y_d[1]\n",
    "            Y = np.concatenate([Y_train, Y_val])\n",
    "            Y_fine = np.concatenate([Y_train_fine, Y_val_fine])\n",
    "\n",
    "            best_model, best_config, best_score = grid_search(LMsnorkel, \n",
    "                                                   model_class_init, \n",
    "                                                   paramgrid,\n",
    "                                                   train = (L_train, Y_train_fine),\n",
    "                                                   dev = (L_val, Y_val_fine),\n",
    "                                                   n_model_search=n_model_search, \n",
    "                                                   val_metric='f1_macro', \n",
    "                                                   seed=1234,\n",
    "                                                   tag_fmt_ckpnt='IO')\n",
    "\n",
    "\n",
    "            if best_score['f1_macro'] > best_f1_macro:\n",
    "                best_f1_macro = best_score['f1_macro']\n",
    "                best_overall_model = best_model\n",
    "                best_overall_config = best_config\n",
    "                best_overall_partition = i\n",
    "                overall_L = L\n",
    "                gold_labels = Y_d[0]\n",
    "                gold_labels_fine = Y_d[1]\n",
    "\n",
    "\n",
    "            print('Best overall macro F1 score: ', best_f1_macro)\n",
    "            print('Best overall configuration: ', best_overall_config)\n",
    "\n",
    "\n",
    "        print('Save the best overall model, configuration and partition for this experiment level')\n",
    "        # Save your model or results\n",
    "        save_dir = f'/mnt/nas2/results/Results/systematicReview/distant_pico/models/LabelModels/{picos}/v4/{l}/'\n",
    "        filename = 'stpartition_' + str(best_overall_partition+1) + '_epoch_' + str(best_config['n_epochs'])\n",
    "        joblib.dump(best_overall_model, f'{save_dir}/{filename}.pkl') \n",
    "        joblib.dump(best_overall_config, f'{save_dir}/{filename}.json')\n",
    "\n",
    "        #load your model for further usage\n",
    "        loaded_best_model = joblib.load(f'{save_dir}/{filename}.pkl')\n",
    "\n",
    "        # Initialize token predictions here\n",
    "        token_predictions = dict()\n",
    "\n",
    "        # Predict on the training set\n",
    "        for i, partition in enumerate(partitioned_d_umls):\n",
    "\n",
    "            if i == best_overall_partition:\n",
    "\n",
    "                # Predict on the test ebm correct set\n",
    "                test_corr_probas = predict(partition, test_corr_cands, loaded_best_model, Y_d[-2], l) # test ebm correct   \n",
    "\n",
    "                # Predict on the test ebm set\n",
    "                test_ebm_probas = predict(partition, test_cands,loaded_best_model, Y_d[-4], l) # test ebm\n",
    "                test_ebm_probas_coarse = predict(partition, test_cands,loaded_best_model, Y_d[-5], l) # test ebm coarse grained\n",
    "\n",
    "                # Predict on the training set\n",
    "                train_probas = predict(partition, train_cands,loaded_best_model, Y_d[-6], l) # train \n",
    "\n",
    "                # Write training predictions to file\n",
    "                # tokens\tpos\toffsets\tlabels\ttrue_labels\n",
    "                train_probas = [list(tp) for tp in train_probas]\n",
    "                train_probas_series = pd.Series(list(train_probas))\n",
    "                data_df['labels'] = train_probas_series.values\n",
    "                \n",
    "                # Write predictions on the training data to the file\n",
    "                write_df = data_df.groupby(['series'])[['series', 'tokens', 'pos', 'offsets', 'labels', str(picos), str(picos)+'_f']].agg(list)\n",
    "                write_file_path = f'/mnt/nas2/results/Results/systematicReview/distant_pico/predictions/LabelModels/{picos}/v4/{l}/{filename}_bestmodel.tsv'\n",
    "                write_df.to_csv (write_file_path, index = None, sep = '\\t', header=True) \n",
    "\n",
    "                # Write predictions for the ebm and ebm corrected files\n",
    "                #token_predictions = dict()\n",
    "                #for counter, (T, P, Y_d_i, Y_d_i_fine) in enumerate(zip(tokens, predictions, gold_labels, gold_labels_fine)):\n",
    "                #    token_predictions[counter] = [T, list(P), list(P).index(max(list(P))), str(Y_d_i), str(Y_d_i_fine)]\n",
    "\n",
    "        #with open(f'{save_dir}/{filename}_results.json', 'w+') as fp:\n",
    "        #    json.dump(token_predictions, fp)\n",
    "            \n",
    "    #return token_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81dac7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparamater Search Space: 336\n",
      "Executing the experiment level:  UMLS\n",
      "Total number of UMLS partitions:  3\n",
      "Grid search over 50 configs\n",
      "[0] Label Model\n",
      "{'lr': 0.001, 'l2': 0.0001, 'n_epochs': 200, 'prec_init': 0.8, 'optimizer': 'adam', 'lr_scheduler': 'constant', 'seed': 1234}\n",
      "[TRAIN] accuracy: 93.16 | precision: 23.28 | recall: 29.18 | f1: 25.90 | f1_macro: 61.16\n",
      "[DEV]   accuracy: 93.25 | precision: 21.70 | recall: 29.95 | f1: 25.17 | f1_macro: 60.82\n",
      "----------------------------------------------------------------------------------------\n",
      "[1] Label Model\n",
      "[2] Label Model\n",
      "[3] Label Model\n",
      "[4] Label Model\n",
      "[5] Label Model\n",
      "[6] Label Model\n",
      "[7] Label Model\n",
      "[8] Label Model\n",
      "[9] Label Model\n",
      "[10] Label Model\n",
      "[11] Label Model\n",
      "[12] Label Model\n",
      "[13] Label Model\n",
      "[14] Label Model\n",
      "[15] Label Model\n",
      "[16] Label Model\n",
      "[17] Label Model\n",
      "[18] Label Model\n",
      "[19] Label Model\n",
      "[20] Label Model\n",
      "[21] Label Model\n",
      "[22] Label Model\n",
      "[23] Label Model\n",
      "[24] Label Model\n",
      "[25] Label Model\n",
      "[26] Label Model\n",
      "[27] Label Model\n",
      "[28] Label Model\n",
      "[29] Label Model\n",
      "[30] Label Model\n",
      "[31] Label Model\n",
      "[32] Label Model\n",
      "[33] Label Model\n",
      "[34] Label Model\n",
      "[35] Label Model\n",
      "[36] Label Model\n",
      "[37] Label Model\n",
      "[38] Label Model\n",
      "[39] Label Model\n",
      "[40] Label Model\n",
      "[41] Label Model\n",
      "[42] Label Model\n",
      "[43] Label Model\n",
      "[44] Label Model\n",
      "[45] Label Model\n",
      "[46] Label Model\n",
      "[47] Label Model\n",
      "[48] Label Model\n",
      "[49] Label Model\n",
      "BEST\n",
      "{'lr': 0.001, 'l2': 0.0001, 'n_epochs': 200, 'prec_init': 0.8, 'optimizer': 'adam', 'lr_scheduler': 'constant', 'seed': 1234}\n",
      "Best overall macro F1 score:  0.6081647308740257\n",
      "Best overall configuration:  {'lr': 0.001, 'l2': 0.0001, 'n_epochs': 200, 'prec_init': 0.8, 'optimizer': 'adam', 'lr_scheduler': 'constant', 'seed': 1234}\n",
      "Total number of UMLS partitions:  4\n",
      "Grid search over 50 configs\n",
      "[0] Label Model\n",
      "{'lr': 0.001, 'l2': 0.0001, 'n_epochs': 200, 'prec_init': 0.8, 'optimizer': 'adam', 'lr_scheduler': 'constant', 'seed': 1234}\n",
      "[TRAIN] accuracy: 93.02 | precision: 22.87 | recall: 29.67 | f1: 25.83 | f1_macro: 61.08\n",
      "[DEV]   accuracy: 93.12 | precision: 21.43 | recall: 30.62 | f1: 25.21 | f1_macro: 60.80\n",
      "----------------------------------------------------------------------------------------\n",
      "[1] Label Model\n",
      "{'lr': 0.001, 'l2': 0.001, 'n_epochs': 100, 'prec_init': 0.9, 'optimizer': 'adam', 'lr_scheduler': 'constant', 'seed': 1234}\n",
      "[TRAIN] accuracy: 93.63 | precision: 24.40 | recall: 26.45 | f1: 25.38 | f1_macro: 61.03\n",
      "[DEV]   accuracy: 93.79 | precision: 23.05 | recall: 27.28 | f1: 24.99 | f1_macro: 60.87\n",
      "----------------------------------------------------------------------------------------\n",
      "[2] Label Model\n",
      "[3] Label Model\n",
      "[4] Label Model\n",
      "[5] Label Model\n",
      "{'lr': 0.0001, 'l2': 0.001, 'n_epochs': 700, 'prec_init': 0.6, 'optimizer': 'adamax', 'lr_scheduler': 'constant', 'seed': 1234}\n",
      "[TRAIN] accuracy: 93.89 | precision: 25.53 | recall: 25.62 | f1: 25.57 | f1_macro: 61.19\n",
      "[DEV]   accuracy: 94.04 | precision: 23.97 | recall: 26.31 | f1: 25.09 | f1_macro: 60.99\n",
      "----------------------------------------------------------------------------------------\n",
      "[6] Label Model\n",
      "[7] Label Model\n",
      "[8] Label Model\n",
      "[9] Label Model\n",
      "[10] Label Model\n",
      "[11] Label Model\n",
      "[12] Label Model\n",
      "[13] Label Model\n",
      "[14] Label Model\n",
      "[15] Label Model\n",
      "[16] Label Model\n",
      "[17] Label Model\n",
      "[18] Label Model\n",
      "[19] Label Model\n",
      "[20] Label Model\n",
      "[21] Label Model\n",
      "[22] Label Model\n",
      "[23] Label Model\n",
      "[24] Label Model\n",
      "[25] Label Model\n",
      "[26] Label Model\n",
      "[27] Label Model\n",
      "[28] Label Model\n",
      "[29] Label Model\n",
      "[30] Label Model\n",
      "[31] Label Model\n",
      "[32] Label Model\n",
      "[33] Label Model\n",
      "[34] Label Model\n",
      "[35] Label Model\n",
      "[36] Label Model\n",
      "[37] Label Model\n",
      "[38] Label Model\n",
      "[39] Label Model\n",
      "[40] Label Model\n",
      "[41] Label Model\n",
      "[42] Label Model\n",
      "[43] Label Model\n",
      "[44] Label Model\n",
      "[45] Label Model\n",
      "[46] Label Model\n",
      "[47] Label Model\n",
      "[48] Label Model\n",
      "[49] Label Model\n",
      "BEST\n",
      "{'lr': 0.0001, 'l2': 0.001, 'n_epochs': 700, 'prec_init': 0.6, 'optimizer': 'adamax', 'lr_scheduler': 'constant', 'seed': 1234}\n",
      "Best overall macro F1 score:  0.6099364403748866\n",
      "Best overall configuration:  {'lr': 0.0001, 'l2': 0.001, 'n_epochs': 700, 'prec_init': 0.6, 'optimizer': 'adamax', 'lr_scheduler': 'constant', 'seed': 1234}\n",
      "Total number of UMLS partitions:  5\n",
      "Grid search over 50 configs\n",
      "[0] Label Model\n",
      "{'lr': 0.001, 'l2': 0.0001, 'n_epochs': 200, 'prec_init': 0.8, 'optimizer': 'adam', 'lr_scheduler': 'constant', 'seed': 1234}\n",
      "[TRAIN] accuracy: 92.88 | precision: 22.39 | recall: 29.94 | f1: 25.62 | f1_macro: 60.94\n",
      "[DEV]   accuracy: 92.97 | precision: 21.01 | recall: 30.96 | f1: 25.03 | f1_macro: 60.67\n",
      "----------------------------------------------------------------------------------------\n",
      "[1] Label Model\n",
      "{'lr': 0.001, 'l2': 0.001, 'n_epochs': 100, 'prec_init': 0.9, 'optimizer': 'adam', 'lr_scheduler': 'constant', 'seed': 1234}\n",
      "[TRAIN] accuracy: 93.19 | precision: 23.29 | recall: 28.89 | f1: 25.79 | f1_macro: 61.11\n",
      "[DEV]   accuracy: 93.30 | precision: 21.83 | recall: 29.76 | f1: 25.18 | f1_macro: 60.84\n",
      "----------------------------------------------------------------------------------------\n",
      "[2] Label Model\n",
      "[3] Label Model\n",
      "[4] Label Model\n",
      "[5] Label Model\n",
      "{'lr': 0.0001, 'l2': 0.001, 'n_epochs': 700, 'prec_init': 0.6, 'optimizer': 'adamax', 'lr_scheduler': 'constant', 'seed': 1234}\n",
      "[TRAIN] accuracy: 93.68 | precision: 25.00 | recall: 27.19 | f1: 26.05 | f1_macro: 61.37\n",
      "[DEV]   accuracy: 93.81 | precision: 23.45 | recall: 27.99 | f1: 25.52 | f1_macro: 61.15\n",
      "----------------------------------------------------------------------------------------\n",
      "[6] Label Model\n",
      "[7] Label Model\n",
      "[8] Label Model\n",
      "[9] Label Model\n",
      "[10] Label Model\n",
      "[11] Label Model\n",
      "[12] Label Model\n",
      "[13] Label Model\n",
      "[14] Label Model\n",
      "[15] Label Model\n",
      "[16] Label Model\n",
      "[17] Label Model\n",
      "[18] Label Model\n",
      "[19] Label Model\n",
      "[20] Label Model\n",
      "[21] Label Model\n",
      "[22] Label Model\n",
      "[23] Label Model\n",
      "[24] Label Model\n",
      "[25] Label Model\n",
      "[26] Label Model\n",
      "[27] Label Model\n",
      "[28] Label Model\n",
      "[29] Label Model\n",
      "[30] Label Model\n",
      "[31] Label Model\n",
      "[32] Label Model\n",
      "[33] Label Model\n",
      "[34] Label Model\n",
      "[35] Label Model\n",
      "[36] Label Model\n",
      "[37] Label Model\n",
      "[38] Label Model\n",
      "[39] Label Model\n",
      "[40] Label Model\n",
      "[41] Label Model\n",
      "[42] Label Model\n",
      "[43] Label Model\n",
      "[44] Label Model\n",
      "[45] Label Model\n",
      "[46] Label Model\n",
      "[47] Label Model\n",
      "[48] Label Model\n",
      "[49] Label Model\n",
      "BEST\n",
      "{'lr': 0.0001, 'l2': 0.001, 'n_epochs': 700, 'prec_init': 0.6, 'optimizer': 'adamax', 'lr_scheduler': 'constant', 'seed': 1234}\n",
      "Best overall macro F1 score:  0.6114704568752539\n",
      "Best overall configuration:  {'lr': 0.0001, 'l2': 0.001, 'n_epochs': 700, 'prec_init': 0.6, 'optimizer': 'adamax', 'lr_scheduler': 'constant', 'seed': 1234}\n"
     ]
    }
   ],
   "source": [
    "predicted_p = train(partitioned_p_umls_fuzzy, train_candidates, test_ebm_candidates, test_ebm_corr_candidates, Y_p, 'p', paramgrid = param_grid, mode = 'pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0accd258",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f09163",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19a8993",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c68e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b25d0e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e350ead",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b56da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df.p.to_numpy() , # 0 -7\n",
    "# data_df.p_f.to_numpy() , # 1 -6\n",
    "# test_ebm_data.p.to_numpy() , # 2 -5\n",
    "# test_ebm_data.p_f.to_numpy() , # 3 -4\n",
    "# test_physio_data.p.to_numpy(),  # 4 -3\n",
    "# test_ebm_corr_df.p.to_numpy(),   # 5 -2\n",
    "# test_ebm_corr_df.p_f.to_numpy() # 6 -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95036abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_i_series = pd.Series(predicted_i)\n",
    "predicted_i_train , predicted_i_test  = [i.to_dict() for i in train_test_split(predicted_i_series, test_size=0.2, shuffle=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c47a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/nas2/results/Results/systematicReview/distant_pico/predictions/LabelModels/i/v3/bestmodel.tsv', 'w+') as wf:\n",
    "    for k,v in predicted_i_test.items():\n",
    "        string2write = str(k) + '\\t' + str(v[0]) + '\\t' + str(v[1]) + '\\t' + str(v[2]) + '\\t' + str(v[3]) + '\\t' + str(v[4]) + '\\n'\n",
    "        wf.write(string2write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41474575",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_p = train(partitioned_p_umls_fuzzy, umls_p[1], nonumls_p[1], ds_p[1], heur_p[0], dict_p[1], Y_p, 'p', paramgrid = param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6053df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "predicted_p_series = pd.Series(predicted_p)\n",
    "predicted_p_train , predicted_p_test  = [i.to_dict() for i in train_test_split(predicted_p_series, test_size=0.2, shuffle=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4699749",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/mnt/nas2/results/Results/systematicReview/distant_pico/predictions/LabelModels/p/v3/bestmodel.tsv', 'w+') as wf:\n",
    "    for k,v in predicted_p_test.items():\n",
    "        string2write = str(k) + '\\t' + str(v[0]) + '\\t' + str(v[1]) + '\\t' + str(v[2]) + '\\t' + str(v[3]) + '\\t' + str(v[4]) + '\\n'\n",
    "        wf.write(string2write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6887c291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the probablistic labels and the meta-data to file\n",
    "tokens_write = np.array( [i[0] for i in predicted_p.values()]  )\n",
    "series_write = Y = np.concatenate([series[0], series[1]])\n",
    "part_of_speech_write = Y = np.concatenate([part_of_speech[0], part_of_speech[1]])\n",
    "offsets_write = Y = np.concatenate([offsets[0], offsets[1]])\n",
    "pred_proba_p_labels_write = [ i[1] for i in predicted_p.values() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89d08b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_p_gold_write = np.concatenate([Y_p[0], Y_p[1]])\n",
    "\n",
    "corpus_df = pd.DataFrame(data=[series_write, tokens_write, part_of_speech_write, offsets_write, pred_proba_p_labels_write, Y_p[0], Y_p[1]]).T\n",
    "corpus_df.columns =['series', 'tokens', 'pos', 'offsets', 'pred_probas', 'true_labels', 'true_labels_fine']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ee491a",
   "metadata": {},
   "outputs": [],
   "source": [
    "series_set = list(set(corpus_df.series))\n",
    "\n",
    "tokens = list(corpus_df.tokens)\n",
    "offsets = list(corpus_df.offsets)\n",
    "pos = list(corpus_df.pos)\n",
    "pred_probas = list(corpus_df.pred_probas)\n",
    "true_labels = list(corpus_df.true_labels)\n",
    "true_labels_fine = list(corpus_df.true_labels_fine)\n",
    "\n",
    "\n",
    "tokens_nest = []\n",
    "offsets_nest = []\n",
    "pos_nest = []\n",
    "pred_probas_nest = []\n",
    "true_labels_nest = []\n",
    "true_labels_fine_nest = []\n",
    "\n",
    "for series_i in series_set:\n",
    "\n",
    "    indices = [i for i, x in enumerate( list( corpus_df.series ) ) if x == series_i]\n",
    "\n",
    "    tokens_nest.append( tokens[ indices[0] : indices[-1]+1 ] )\n",
    "    offsets_nest.append( offsets[ indices[0] : indices[-1]+1 ] )\n",
    "    pos_nest.append( pos[ indices[0] : indices[-1]+1 ] )\n",
    "    pred_probas_nest.append( pred_probas[ indices[0] : indices[-1]+1 ] )\n",
    "    true_labels_nest.append( true_labels[ indices[0] : indices[-1]+1 ] )\n",
    "    true_labels_fine_nest.append( true_labels_fine[ indices[0] : indices[-1]+1 ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe4ce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_p_df = pd.DataFrame(data=[tokens_nest, pos_nest, offsets_nest, pred_probas_nest, true_labels_nest, true_labels_fine_nest]).T\n",
    "write_p_df.columns = ['tokens', 'pos', 'offsets', 'labels', 'true_labels', 'true_labels_fine']\n",
    "\n",
    "write_p_df.to_csv(f'/mnt/nas2/results/Results/systematicReview/distant_pico/predictions/LabelModels/p/v3/bestmodel.tsv', sep='\\t', encoding='utf-8', header='true')\n",
    "\n",
    "token_list = [num for sublist in tokens_nest for num in sublist]\n",
    "pred_probas_list = [num for sublist in pred_probas_nest for num in sublist]\n",
    "pred_list = [np.argmax(np.asarray(num)) for sublist in pred_probas_nest for num in sublist]\n",
    "true_list = [num for sublist in true_labels_nest for num in sublist]\n",
    "true_fine_list = [num for sublist in true_labels_fine_nest for num in sublist]\n",
    "\n",
    "ea_p_df = pd.DataFrame(data=[token_list, pred_probas_list, pred_list, true_list, true_fine_list]).T\n",
    "ea_p_df.columns = ['tokens', 'label_probas', 'labels', 'true_labels', 'true_labels_fine']\n",
    "_, val_ea_p_df = train_test_split(ea_p_df, test_size=0.2, shuffle=False)\n",
    "\n",
    "\n",
    "val_ea_p_df.to_csv(f'/mnt/nas2/results/Results/systematicReview/distant_pico/predictions/LabelModels/p/v3/bestmodel_ea.tsv', sep='\\t', encoding='utf-8', header='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723ae8da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf459f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875e371f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480453b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e67d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "mypath_P = '/mnt/nas2/data/systematicReview/ebm_nlp_2_00/annotations/aggregated/starting_spans/participants/test/gold'\n",
    "onlyfiles_P = [f for f in listdir(mypath_P) if isfile(join(mypath_P, f))]\n",
    "\n",
    "mypath_I = '/mnt/nas2/data/systematicReview/ebm_nlp_2_00/annotations/aggregated/starting_spans/interventions/test/gold'\n",
    "onlyfiles_I = [f for f in listdir(mypath_I) if isfile(join(mypath_I, f))]\n",
    "\n",
    "mypath_O = '/mnt/nas2/data/systematicReview/ebm_nlp_2_00/annotations/aggregated/starting_spans/outcomes/test/gold'\n",
    "onlyfiles_O = [f for f in listdir(mypath_O) if isfile(join(mypath_O, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a708384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyfiles = onlyfiles_P + onlyfiles_I + onlyfiles_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bda0479",
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyfiles = list(set(onlyfiles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e50fd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "onlyfiles_clean = [f.replace('.AGGREGATED.ann', '') for f in onlyfiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abab66e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydir = '/mnt/nas2/data/systematicReview/ebm_nlp_2_00/documents/'\n",
    "all_docs = [f for f in listdir(mydir) if isfile(join(mydir, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731ead52",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ebm = []\n",
    "\n",
    "for count, d in enumerate(all_docs):\n",
    "    if 'txt' in d and str(d).replace('.txt', '') in onlyfiles_clean:\n",
    "        test_ebm.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6254df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe2be44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
