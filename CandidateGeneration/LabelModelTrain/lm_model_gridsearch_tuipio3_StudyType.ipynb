{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b3b4e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "import glob\n",
    "import os\n",
    "from hashlib import new\n",
    "from pathlib import Path\n",
    "import time\n",
    "from itertools import product\n",
    "\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from snorkel.labeling.model import LabelModel\n",
    "from snorkel.labeling import PandasLFApplier\n",
    "\n",
    "from snorkel.labeling.model import MajorityLabelVoter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b79b8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "candgen_version = 'v4' # version = {v3, v4, ...}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cd54fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7798c0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d1d2219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list2Nested(l, nested_length):\n",
    "    return [l[i:i+nested_length] for i in range(0, len(l), nested_length)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daaad8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelModel_mapper_LF = {1:1, -1:0, 0:-1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0638a0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import LMutils\n",
    "\n",
    "train_file = f'/mnt/nas2/results/Results/systematicReview/distant_pico/EBM_PICO_GT/{candgen_version}/gt/train_ebm_labels_tui_pio3.tsv'\n",
    "training_data = pd.read_csv(train_file, sep='\\t', header=0)\n",
    "\n",
    "ebm_test_file = f'/mnt/nas2/results/Results/systematicReview/distant_pico/EBM_PICO_GT/{candgen_version}/gt/test_ebm_labels_tui_pio3.tsv'\n",
    "test_ebm_data = pd.read_csv(ebm_test_file, sep='\\t', header=0)\n",
    "test_ebm_data.rename( columns={'Unnamed: 0':'series'}, inplace=True )\n",
    "\n",
    "physio_test_file = f'/mnt/nas2/results/Results/systematicReview/distant_pico/EBM_PICO_GT/{candgen_version}/gt/test_physio_labels_tui_pio3.tsv'\n",
    "test_physio_data = pd.read_csv(physio_test_file, sep='\\t', header=0)\n",
    "test_physio_data.rename( columns={'Unnamed: 0':'series'}, inplace=True )\n",
    "\n",
    "ebm_test_corrected_file = f'/mnt/nas2/results/Results/systematicReview/distant_pico/EBM_PICO_GT/{candgen_version}/gt/test_ebm_correctedlabels_tui_pio3.tsv'\n",
    "test_ebm_corrected_data = pd.read_csv(ebm_test_corrected_file, sep='\\t', header=0)\n",
    "test_ebm_corrected_data.rename( columns={'Unnamed: 0':'series'}, inplace=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fff28521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_df(df):\n",
    "\n",
    "    df_series = [ index for index, value in df.tokens.items() for word in ast.literal_eval(value) ]\n",
    "    df_tokens = [ word for index, value in df.tokens.items() for word in ast.literal_eval(value) ]\n",
    "    df_pos = [ word for index, value in df.pos.items() for word in ast.literal_eval(value) ]\n",
    "    df_offsets = [ word for index, value in df.offsets.items() for word in ast.literal_eval(value) ]\n",
    "\n",
    "\n",
    "    df_p = [ int(lab) for index, value in df.p.items() for lab in ast.literal_eval(value) ]\n",
    "    df_p_fine = [ int(lab) for index, value in df.p_f.items() for lab in ast.literal_eval(value) ]\n",
    "    df_i = [ int(lab) for index, value in df.i.items() for lab in ast.literal_eval(value) ]\n",
    "    df_i_fine = [ int(lab) for index, value in df.i_f.items() for lab in ast.literal_eval(value) ]\n",
    "    df_o = [ int(lab) for index, value in df.o.items() for lab in ast.literal_eval(value) ]\n",
    "    df_o_fine = [ int(lab) for index, value in df.o_f.items() for lab in ast.literal_eval(value) ]\n",
    "    df_s = [ int(lab) for index, value in df.s.items() for lab in ast.literal_eval(value) ]\n",
    "    df_s_fine = [ int(lab) for index, value in df.s_f.items() for lab in ast.literal_eval(value) ]\n",
    "    \n",
    "    df_flattened = pd.DataFrame({ 'series': df_series,\n",
    "                        'tokens' : df_tokens,\n",
    "                        'offsets': df_offsets,\n",
    "                        'pos': df_pos,\n",
    "                        'p' : df_p,\n",
    "                        'i' : df_i,\n",
    "                        'o' : df_o,\n",
    "                        's' : df_s,\n",
    "                        'p_f' : df_p_fine,\n",
    "                        'i_f' : df_i_fine,\n",
    "                        'o_f' : df_o_fine,\n",
    "                        's_f' : df_s_fine})\n",
    "    \n",
    "    return df_flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b8e07f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the dataframes (currently only the training dataframe and test ebm dataframe with corrected labels can be flattened)\n",
    "data_df = flatten_df(training_data)\n",
    "test_ebm_data = flatten_df(test_ebm_data)\n",
    "test_ebm_corr_df = flatten_df(test_ebm_corrected_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0c041c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "series = [\n",
    "    data_df.series.to_numpy() ,\n",
    "    test_ebm_data.series.to_numpy() ,\n",
    "    test_physio_data.series.to_numpy(),   \n",
    "    test_ebm_corr_df.series.to_numpy()\n",
    "]\n",
    "\n",
    "\n",
    "sents = [\n",
    "    data_df.tokens.to_numpy() ,\n",
    "    test_ebm_data.tokens.to_numpy() ,\n",
    "    test_physio_data.tokens.to_numpy(),   \n",
    "    test_ebm_corr_df.tokens.to_numpy()    \n",
    "]\n",
    "\n",
    "\n",
    "part_of_speech = [\n",
    "    data_df.pos.to_numpy() ,\n",
    "    test_ebm_data.pos.to_numpy() ,\n",
    "    test_physio_data.pos.to_numpy(),   \n",
    "    test_ebm_corr_df.pos.to_numpy()     \n",
    "]\n",
    "\n",
    "\n",
    "offsets = [\n",
    "    data_df.offsets.to_numpy() ,\n",
    "    test_ebm_data.offsets.to_numpy() ,\n",
    "    test_physio_data.offsets.to_numpy(),   \n",
    "    test_ebm_corr_df.offsets.to_numpy() \n",
    "]\n",
    "\n",
    "\n",
    "Y_p = [\n",
    "    data_df.p.to_numpy() , # 0 -7\n",
    "    data_df.p_f.to_numpy() , # 1 -6\n",
    "    test_ebm_data.p.to_numpy() , # 2 -5\n",
    "    test_ebm_data.p_f.to_numpy() , # 3 -4\n",
    "    test_physio_data.p.to_numpy(),  # 4 -3\n",
    "    test_ebm_corr_df.p.to_numpy(),   # 5 -2\n",
    "    test_ebm_corr_df.p_f.to_numpy() # 6 -1\n",
    "]\n",
    "\n",
    "\n",
    "Y_i = [\n",
    "    data_df.i.to_numpy() , # 0 -7\n",
    "    data_df.i_f.to_numpy() , # 1 -6\n",
    "    test_ebm_data.i.to_numpy() , # 2 -5\n",
    "    test_ebm_data.i_f.to_numpy() , # 3 -4\n",
    "    test_physio_data.i.to_numpy(),  # 4 -3\n",
    "    test_ebm_corr_df.i.to_numpy(),   # 5 -2\n",
    "    test_ebm_corr_df.i_f.to_numpy() # 6 -1\n",
    "]\n",
    "\n",
    "\n",
    "Y_o = [\n",
    "    data_df.o.to_numpy() ,\n",
    "    data_df.o_f.to_numpy() ,\n",
    "    test_ebm_data.o.to_numpy() ,\n",
    "    test_physio_data.o.to_numpy() \n",
    "]\n",
    "\n",
    "Y_s = [\n",
    "    data_df.s.to_numpy() , # 0 -7\n",
    "    data_df.s_f.to_numpy() , # 1 -6\n",
    "    test_ebm_data.s.to_numpy() , # 2 -5\n",
    "    test_ebm_data.s_f.to_numpy() , # 3 -4\n",
    "    test_physio_data.s.to_numpy(),  # 4 -3\n",
    "    test_ebm_corr_df.s.to_numpy(),   # 5 -2\n",
    "    test_ebm_corr_df.s_f.to_numpy() # 6 -1\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd0f16cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data for error analysis\n",
    "\n",
    "error_analysis_ebm_p = pd.DataFrame({'tokens' : test_ebm_data.tokens,\n",
    "                                'participant' : test_ebm_data.p,\n",
    "                                'participant_fine' : test_ebm_data.p_f }, \n",
    "                                columns=['tokens','participant', 'participant_fine'])\n",
    "\n",
    "#error_analysis_ebm_p.to_csv (r'/mnt/nas2/results/Results/systematicReview/distant_pico/EBM_PICO_GT/error_analysis/test_ebmgold_p', index = None, header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4c4068d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write data for error analysis\n",
    "\n",
    "error_analysis_ebmcorr_p = pd.DataFrame({'tokens' : test_ebm_corr_df.tokens,\n",
    "                                'participant' : test_ebm_corr_df.p,\n",
    "                                'participant_fine' : test_ebm_corr_df.p_f }, \n",
    "                                columns=['tokens','participant', 'participant_fine'])\n",
    "\n",
    "#error_analysis_ebmcorr_p.to_csv (r'/mnt/nas2/results/Results/systematicReview/distant_pico/EBM_PICO_GT/error_analysis/test_ebmgoldcorr_p', index = None, header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0c7fe1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_list(data_column):\n",
    "    return [ word for index, value in data_column.items() for word in ast.literal_eval(value) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35850d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_array(data_column):\n",
    "    return np.array( [ word for index, value in data_column.items() for word in ast.literal_eval(value) ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9f6e149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dict_to_array(label_column):\n",
    "    return np.array( [ labelModel_mapper_LF[int(lab)] for index, value in label_column.items() for k, lab in ast.literal_eval(value).items() ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b873406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lfs(indir):\n",
    "    \n",
    "    pathlist = Path(indir).glob('**/*.tsv')\n",
    "\n",
    "    tokens = ''\n",
    "\n",
    "    lfs = dict()\n",
    "    lfs_lm = dict()\n",
    "\n",
    "    for counter, file in enumerate(pathlist):\n",
    "        \n",
    "        if '/S/' in str(file):\n",
    "\n",
    "            k = str( file ).split(f'/{candgen_version}/')[-1].replace('.tsv', '').replace('/', '_')\n",
    "            mypath = Path(file)\n",
    "            if mypath.stat().st_size != 0:\n",
    "                data = pd.read_csv(file, sep='\\t', header=0)\n",
    "\n",
    "                data_tokens = data.tokens\n",
    "                if len(tokens) < 5:\n",
    "                    tokens = df_to_array(data_tokens)\n",
    "\n",
    "                data_labels = data.labels\n",
    "                #print(len(data_labels))\n",
    "                labels = dict_to_array(data_labels)\n",
    "                #print(len(labels))\n",
    "                if len(labels) != len(tokens):\n",
    "                    print(k, len(labels) , len(tokens) )\n",
    "                #assert len(labels) == len(tokens)\n",
    "                lfs[k] = labels\n",
    "\n",
    "\n",
    "    print( 'Total number of tokens in validation set: ', len(tokens) )\n",
    "    print( 'Total number of LFs in the dictionary', len(lfs) )\n",
    "    \n",
    "    return lfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1119c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in validation set:  1303169\n",
      "Total number of LFs in the dictionary 6\n"
     ]
    }
   ],
   "source": [
    "indir = f'/mnt/nas2/results/Results/systematicReview/distant_pico/training_ebm_candidate_generation/{candgen_version}'\n",
    "train_ebm_lfs = get_lfs(indir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "456a19e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in validation set:  52582\n",
      "Total number of LFs in the dictionary 6\n"
     ]
    }
   ],
   "source": [
    "indir_test_ebm_corr = f'/mnt/nas2/results/Results/systematicReview/distant_pico/test_ebm_anjani_candidate_generation/{candgen_version}'\n",
    "test_ebm_corr_lfs = get_lfs(indir_test_ebm_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58ac9849",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens in validation set:  51784\n",
      "Total number of LFs in the dictionary 6\n"
     ]
    }
   ],
   "source": [
    "indir_test_ebm = f'/mnt/nas2/results/Results/systematicReview/distant_pico/test_ebm_candidate_generation/{candgen_version}'\n",
    "test_ebm_lfs = get_lfs(indir_test_ebm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "001d73ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lf_levels(umls_d:dict, pattern:str, picos:str):\n",
    "\n",
    "    umls_level = dict()\n",
    "\n",
    "    for key, value in umls_d.items():   # iter on both keys and values\n",
    "        search_pattern = pattern + picos\n",
    "        if key.startswith(search_pattern):\n",
    "            k = str(key).split('_')[-1]\n",
    "            umls_level[ k ] = value\n",
    "\n",
    "    return umls_level\n",
    "\n",
    "\n",
    "# Level 1: UMLS\n",
    "umls_p = [\n",
    "    lf_levels(train_ebm_lfs, name, 'P') \n",
    "    for i, name in enumerate(['UMLS_direct_', 'UMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "umls_p_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'P') \n",
    "    for i, name in enumerate(['UMLS_direct_', 'UMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "umls_p_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'P') \n",
    "    for i, name in enumerate(['UMLS_direct_', 'UMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "umls_i = [\n",
    "    lf_levels(train_ebm_lfs, name, 'I') \n",
    "    for i, name in enumerate(['UMLS_direct_', 'UMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "umls_i_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'I') \n",
    "    for i, name in enumerate(['UMLS_direct_', 'UMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "umls_i_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'I') \n",
    "    for i, name in enumerate(['UMLS_direct_', 'UMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "umls_o = [\n",
    "    lf_levels(train_ebm_lfs, name, 'O') \n",
    "    for i, name in enumerate(['UMLS_direct_', 'UMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "umls_o_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'O') \n",
    "    for i, name in enumerate(['UMLS_direct_', 'UMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "umls_o_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'O') \n",
    "    for i, name in enumerate(['UMLS_direct_', 'UMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Level 2: non UMLS\n",
    "nonumls_p = [\n",
    "    lf_levels(train_ebm_lfs, name, 'P') \n",
    "    for i, name in enumerate(['nonUMLS_direct_', 'nonUMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "nonumls_p_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'P') \n",
    "    for i, name in enumerate(['nonUMLS_direct_', 'nonUMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "nonumls_p_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'P') \n",
    "    for i, name in enumerate(['nonUMLS_direct_', 'nonUMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "nonumls_i = [\n",
    "    lf_levels(train_ebm_lfs, name, 'I') \n",
    "    for i, name in enumerate(['nonUMLS_direct_', 'nonUMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "nonumls_i_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'I') \n",
    "    for i, name in enumerate(['nonUMLS_direct_', 'nonUMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "nonumls_i_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'I') \n",
    "    for i, name in enumerate(['nonUMLS_direct_', 'nonUMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "nonumls_o = [\n",
    "    lf_levels(train_ebm_lfs, name, 'O') \n",
    "    for i, name in enumerate(['nonUMLS_direct_', 'nonUMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "nonumls_o_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'O') \n",
    "    for i, name in enumerate(['nonUMLS_direct_', 'nonUMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "nonumls_o_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'O') \n",
    "    for i, name in enumerate(['nonUMLS_direct_', 'nonUMLS_fuzzy_'])\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "# Level 3: DS\n",
    "ds_p = [\n",
    "    lf_levels(train_ebm_lfs, name, 'P') \n",
    "    for i, name in enumerate(['ds_direct_', 'ds_fuzzy_'])\n",
    "]\n",
    "\n",
    "ds_p_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'P') \n",
    "    for i, name in enumerate(['ds_direct_', 'ds_fuzzy_'])\n",
    "]\n",
    "\n",
    "ds_p_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'P') \n",
    "    for i, name in enumerate(['ds_direct_', 'ds_fuzzy_'])\n",
    "]\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "ds_i = [\n",
    "    lf_levels(train_ebm_lfs, name, 'I') \n",
    "    for i, name in enumerate(['ds_direct_', 'ds_fuzzy_'])\n",
    "]\n",
    "\n",
    "ds_i_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'I') \n",
    "    for i, name in enumerate(['ds_direct_', 'ds_fuzzy_'])\n",
    "]\n",
    "\n",
    "ds_i_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'I') \n",
    "    for i, name in enumerate(['ds_direct_', 'ds_fuzzy_'])\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "ds_o = [\n",
    "    lf_levels(train_ebm_lfs, name, 'O') \n",
    "    for i, name in enumerate(['ds_direct_', 'ds_fuzzy_'])\n",
    "]\n",
    "\n",
    "ds_o_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'O') \n",
    "    for i, name in enumerate(['ds_direct_', 'ds_fuzzy_'])\n",
    "]\n",
    "\n",
    "ds_o_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'O') \n",
    "    for i, name in enumerate(['ds_direct_', 'ds_fuzzy_'])\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Level 4: dictionary, rules, heuristics\n",
    "heur_p = [\n",
    "    lf_levels(train_ebm_lfs, name, 'P') \n",
    "    for i, name in enumerate(['heuristics_direct_'])\n",
    "]\n",
    "\n",
    "heur_p_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'P') \n",
    "    for i, name in enumerate(['heuristics_direct_'])\n",
    "]\n",
    "\n",
    "heur_p_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'P') \n",
    "    for i, name in enumerate(['heuristics_direct_'])\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "heur_i = [\n",
    "    lf_levels(train_ebm_lfs, name, 'I') \n",
    "    for i, name in enumerate(['heuristics_direct_'])\n",
    "]\n",
    "\n",
    "heur_i_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'I') \n",
    "    for i, name in enumerate(['heuristics_direct_'])\n",
    "]\n",
    "\n",
    "heur_i_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'I') \n",
    "    for i, name in enumerate(['heuristics_direct_'])\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "heur_o = [\n",
    "    lf_levels(train_ebm_lfs, name, 'O') \n",
    "    for i, name in enumerate(['heuristics_direct_'])\n",
    "]\n",
    "\n",
    "heur_o_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'O') \n",
    "    for i, name in enumerate(['heuristics_direct_'])\n",
    "]\n",
    "\n",
    "\n",
    "heur_o_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'O') \n",
    "    for i, name in enumerate(['heuristics_direct_'])\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "heur_s = [\n",
    "    lf_levels(train_ebm_lfs, name, 'S') \n",
    "    for i, name in enumerate(['heuristics_direct_'])\n",
    "]\n",
    "\n",
    "heur_s_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'S') \n",
    "    for i, name in enumerate(['heuristics_direct_'])\n",
    "]\n",
    "\n",
    "\n",
    "heur_s_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'S') \n",
    "    for i, name in enumerate(['heuristics_direct_'])\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "dict_p = [\n",
    "    lf_levels(train_ebm_lfs, name, 'P') \n",
    "    for i, name in enumerate(['dictionary_direct_', 'dictionary_fuzzy_'])\n",
    "]\n",
    "\n",
    "dict_p_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'P') \n",
    "    for i, name in enumerate(['dictionary_direct_', 'dictionary_fuzzy_'])\n",
    "]\n",
    "\n",
    "dict_p_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'P') \n",
    "    for i, name in enumerate(['dictionary_direct_', 'dictionary_fuzzy_'])\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "dict_i = [\n",
    "    lf_levels(train_ebm_lfs, name, 'I') \n",
    "    for i, name in enumerate(['dictionary_direct_', 'dictionary_fuzzy_'])\n",
    "]\n",
    "\n",
    "dict_i_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'I') \n",
    "    for i, name in enumerate(['dictionary_direct_', 'dictionary_fuzzy_'])\n",
    "]\n",
    "\n",
    "dict_i_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'I') \n",
    "    for i, name in enumerate(['dictionary_direct_', 'dictionary_fuzzy_'])\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "dict_o = [\n",
    "    lf_levels(train_ebm_lfs, name, 'O') \n",
    "    for i, name in enumerate(['dictionary_direct_', 'dictionary_fuzzy_'])\n",
    "]\n",
    "\n",
    "dict_o_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'O') \n",
    "    for i, name in enumerate(['dictionary_direct_', 'dictionary_fuzzy_'])\n",
    "]\n",
    "\n",
    "dict_o_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'O') \n",
    "    for i, name in enumerate(['dictionary_direct_', 'dictionary_fuzzy_'])\n",
    "]\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "dict_s = [\n",
    "    lf_levels(train_ebm_lfs, name, 'S') \n",
    "    for i, name in enumerate(['dictionary_direct_', 'dictionary_fuzzy_'])\n",
    "]\n",
    "\n",
    "dict_s_testcorrected = [\n",
    "    lf_levels(test_ebm_corr_lfs, name, 'S') \n",
    "    for i, name in enumerate(['dictionary_direct_', 'dictionary_fuzzy_'])\n",
    "]\n",
    "\n",
    "dict_s_testebm = [\n",
    "    lf_levels(test_ebm_lfs, name, 'S') \n",
    "    for i, name in enumerate(['dictionary_direct_', 'dictionary_fuzzy_'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdb4ad6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study Type data\n",
    "\n",
    "train_s_candidates = [dict_s[1], heur_s[0]]\n",
    "test_s_ebm_corr_candidates = [dict_s_testcorrected[1], heur_s_testcorrected[0]]\n",
    "test_s_ebm_candidates = [dict_s_testebm[1], heur_s_testebm[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2c6e5471",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'lr': [0.001, 0.0001],\n",
    "    'l2': [0.001, 0.0001],\n",
    "    'n_epochs': [50, 100, 200, 600, 700, 1000, 2000],\n",
    "    'prec_init': [0.6, 0.7, 0.8, 0.9],\n",
    "    'optimizer': [\"adamax\", \"adam\", \"sgd\"],\n",
    "    'lr_scheduler': ['constant'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d083955",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_param_grid(param_grid, seed):\n",
    "    \"\"\" Sample parameter grid\n",
    "    :param param_grid:\n",
    "    :param seed:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    rstate = np.random.get_state()\n",
    "    np.random.seed(seed)\n",
    "    params = list(product(*[param_grid[name] for name in param_grid]))\n",
    "    np.random.shuffle(params)\n",
    "    np.random.set_state(rstate)\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "602295fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(model_class,\n",
    "                model_class_init,\n",
    "                param_grid,\n",
    "                train=None,\n",
    "                dev=None,\n",
    "                other_train=None,\n",
    "                n_model_search=5,\n",
    "                val_metric='f1_macro',\n",
    "                seed=1234,\n",
    "                checkpoint_gt_mv=False,\n",
    "                tag_fmt_ckpnt='IO'):\n",
    "    \n",
    "    \n",
    "    \"\"\"Simple grid search helper function\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_class\n",
    "    model_class_init\n",
    "    param_grid\n",
    "    train\n",
    "    dev\n",
    "    n_model_search\n",
    "    val_metric\n",
    "    seed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    \"\"\"\n",
    "    \n",
    "    #L_train, Y_train = train\n",
    "    #L_dev, Y_dev = dev\n",
    "    L_train = train\n",
    "    L_dev = dev\n",
    "\n",
    "    # sample configs\n",
    "    params = sample_param_grid(param_grid, seed)[:n_model_search]\n",
    "\n",
    "    defaults = {'seed': seed}\n",
    "    best_score, best_config = 0.0, None\n",
    "    print(f\"Grid search over {len(params)} configs\")\n",
    "\n",
    "    for i, config in enumerate(params):\n",
    "        print(f'[{i}] Label Model')\n",
    "        config = dict(zip(param_grid.keys(), config))\n",
    "        config.update({param: value for param, value in defaults.items() if param not in config})\n",
    "\n",
    "        model = model_class(**model_class_init)\n",
    "        model.fit(L_train, Y_dev, **config)\n",
    "        \n",
    "        y_pred = model.predict(L_dev)\n",
    "        \n",
    "        if tag_fmt_ckpnt == 'IO':\n",
    "            y_gold = np.array([0 if y == 0 else 1 for y in Y_dev])\n",
    "        else:\n",
    "            y_gold = Y_dev\n",
    "            \n",
    "            \n",
    "        if -1 in y_pred:\n",
    "            print(\"Label model predicted -1 (TODO: this happens inconsistently)\")\n",
    "            continue\n",
    "            \n",
    "        # use internal label model scorer to score the prediction\n",
    "        metrics = model.score(L=L_dev,\n",
    "                              Y=y_gold,\n",
    "                              metrics=['accuracy', 'precision', 'recall', 'f1', 'f1_macro'],\n",
    "                              tie_break_policy='random')\n",
    "        \n",
    "    \n",
    "        # compare learned model against MV on same labeled dev set\n",
    "        # skip if LM less than MV\n",
    "        if checkpoint_gt_mv:\n",
    "            mv_metrics = model.score(L=L_dev,\n",
    "                                  Y=y_gold,\n",
    "                                  metrics=['accuracy', 'precision', 'recall', 'f1', 'f1_macro'],\n",
    "                                  tie_break_policy='random')\n",
    "\n",
    "            if metrics[val_metric] < mv_metrics[val_metric]:\n",
    "                continue\n",
    "                \n",
    "        if not best_score or metrics[val_metric] > best_score[val_metric]:\n",
    "            print(config)\n",
    "            best_score = metrics\n",
    "            best_config = config\n",
    "            \n",
    "            # print training set score if we have labeled data\n",
    "            if np.any(Y_train):\n",
    "                y_pred = model.predict(L_train)\n",
    "\n",
    "                if tag_fmt_ckpnt == 'IO':\n",
    "                    y_gold = np.array([0 if y == 0 else 1 for y in Y_train])\n",
    "                else:\n",
    "                    y_gold = Y_train\n",
    "\n",
    "                metrics = model.score(L=L_train,\n",
    "                                      Y=y_gold,\n",
    "                                      metrics=['accuracy', 'precision', 'recall', 'f1', 'f1_macro'],\n",
    "                                      tie_break_policy='random')\n",
    "\n",
    "                print('[TRAIN] {}'.format(' | '.join([f'{m}: {v * 100:2.2f}' for m, v in metrics.items()])))\n",
    "\n",
    "            print('[DEV]   {}'.format(' | '.join([f'{m}: {v * 100:2.2f}' for m, v in best_score.items()])))\n",
    "            print('-' * 88)\n",
    "            \n",
    "            \n",
    "    # retrain best model\n",
    "    print('BEST')\n",
    "    print(best_config)\n",
    "    model = model_class(**model_class_init)\n",
    "    \n",
    "    \n",
    "    model.fit(L_train, Y_dev, **best_config)\n",
    "    return model, best_config, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "192e9fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_plus(cands, best_model, gt_labels, mode=None):\n",
    "    \n",
    "    combined_lf = []\n",
    "    combined_lf.extend( list(cands[0].values()) ) # Combine with level 4\n",
    "    combined_lf.extend( list(cands[1].values()) ) # combine with level 4\n",
    "\n",
    "\n",
    "    L = np.array( combined_lf )\n",
    "    L = np.transpose(L)\n",
    "    \n",
    "    if mode == 'only_pred':\n",
    "    \n",
    "        predictions_probablities = best_model.predict_proba(L)\n",
    "        predictions = best_model.predict(L)\n",
    "        \n",
    "        return predictions\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        predictions_probablities = best_model.predict_proba(L)\n",
    "        predictions = best_model.predict(L)\n",
    "    \n",
    "        groundtruth = np.array(gt_labels) \n",
    "\n",
    "        groundtruth = [-1 if x == 0 else x for x in gt_labels] # XXX if \"test_ebm_correct\"\n",
    "        groundtruth = np.array(groundtruth)\n",
    "\n",
    "        groundtruth_ = []\n",
    "        predictions_ = []\n",
    "        for g, p in zip(groundtruth, np.array(predictions)):\n",
    "            if p == 0:\n",
    "                print( 'model predicts 0 inconsistently.' )\n",
    "            else:\n",
    "                groundtruth_.append(g)\n",
    "                predictions_.append(p)\n",
    "\n",
    "\n",
    "\n",
    "        cr = classification_report( groundtruth_, predictions_, digits=4, output_dict=True )\n",
    "        #cr = classification_report( groundtruth_, predictions_, digits=4 )\n",
    "        #print( cr )\n",
    "    \n",
    "        return predictions_probablities, cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b0f2f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for entities that do not have UMLS partitions\n",
    "\n",
    "def train_plus(train_cands, test_cands, test_corr_cands, Y_d, picos, paramgrid, mode):\n",
    "   \n",
    "    gold_labels = ''\n",
    "    gold_labels_fine = ''\n",
    "    \n",
    "    \n",
    "    model_class_init = {\n",
    "        'cardinality': 2, \n",
    "        'verbose': True\n",
    "    }\n",
    "\n",
    "    num_hyperparams = functools.reduce(lambda x,y:x*y, [len(x) for x in param_grid.values()])\n",
    "    print(\"Hyperparamater Search Space:\", num_hyperparams)\n",
    "    n_model_search = 50\n",
    "    \n",
    "\n",
    "    '''#########################################################################\n",
    "    # Choosing the number of LF's from UMLS all\n",
    "    #########################################################################'''\n",
    "\n",
    "\n",
    "    best_f1_macro = 0.0\n",
    "    best_overall_model = ''\n",
    "    best_overall_config = ''\n",
    "\n",
    "    combined_lf = []\n",
    "    combined_lf.extend( list(train_cands[0].values()) ) \n",
    "    combined_lf.extend( list(train_cands[1].values()) )\n",
    "    \n",
    "    L = np.array( combined_lf )\n",
    "    L = np.transpose(L)\n",
    "\n",
    "    # sample configs\n",
    "    params = sample_param_grid(param_grid, 0)[:n_model_search]\n",
    "    defaults = {'seed': 0}\n",
    "    print(f\"Grid search over {len(params)} configs\")\n",
    "\n",
    "    for i, config in enumerate(params):\n",
    "        print(f'[{i}] Label Model')\n",
    "        config = dict(zip(param_grid.keys(), config))\n",
    "        config.update({param: value for param, value in defaults.items() if param not in config})\n",
    "\n",
    "        label_model = LabelModel(cardinality=2, verbose=True)\n",
    "        label_model.fit(L_train=L, **config)\n",
    "\n",
    "        # Predict on the test ebm correct set\n",
    "        preds, class_report = test_corr_probas = predict_plus(test_corr_cands, label_model, Y_d[-2]) # test ebm correct   \n",
    "        \n",
    "        if class_report['macro avg']['f1-score'] > best_f1_macro:\n",
    "            best_f1_macro = class_report['macro avg']['f1-score']\n",
    "            best_overall_model = label_model\n",
    "            best_overall_config = config\n",
    "            \n",
    "    # Save the best label model\n",
    "    print('Save the best overall model, configuration and partition for this experiment level')\n",
    "    # Save your model or results\n",
    "    save_dir = f'/mnt/nas2/results/Results/systematicReview/distant_pico/models/LabelModels/{picos}/'\n",
    "    filename = 'stpartition_' + '_epoch_' + str(best_overall_config['n_epochs'])\n",
    "    joblib.dump(best_overall_model, f'{save_dir}/{filename}.pkl') \n",
    "    joblib.dump(best_overall_config, f'{save_dir}/{filename}.json')\n",
    "    \n",
    "    \n",
    "    #load your model for further usage\n",
    "    loaded_best_model = joblib.load(f'{save_dir}/{filename}.pkl')\n",
    "    \n",
    "    # Predict on the training set\n",
    "    train_probas = predict_plus(train_cands,loaded_best_model, Y_d[-6], mode= 'only_pred') # train \n",
    "    \n",
    "    # Write training predictions to file\n",
    "    # tokens\tpos\toffsets\tlabels\ttrue_labels\n",
    "    #print( train_probas.shape )\n",
    "    train_probas = train_probas.tolist()\n",
    "    #print( len(train_probas) )\n",
    "    #train_probas = [list(tp) for tp in train_probas]\n",
    "    #train_probas_series = pd.Series(list(train_probas))\n",
    "    train_probas_series = pd.Series(train_probas)\n",
    "    data_df['labels'] = train_probas_series.values\n",
    "\n",
    "    # Write predictions on the training data to the file\n",
    "    write_df = data_df.groupby(['series'])[['series', 'tokens', 'pos', 'offsets', 'labels']].agg(list)\n",
    "    write_file_path = f'/mnt/nas2/results/Results/systematicReview/distant_pico/predictions/LabelModels/{picos}/{filename}_bestmodel.tsv'\n",
    "    write_df.to_csv (write_file_path, index = None, sep = '\\t', header=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05079643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparamater Search Space: 336\n",
      "Grid search over 50 configs\n",
      "[0] Label Model\n",
      "model predicts 0 inconsistently.\n",
      "model predicts 0 inconsistently.\n",
      "[1] Label Model\n",
      "model predicts 0 inconsistently.\n",
      "model predicts 0 inconsistently.\n",
      "[2] Label Model\n",
      "[3] Label Model\n",
      "[4] Label Model\n",
      "model predicts 0 inconsistently.\n",
      "model predicts 0 inconsistently.\n",
      "[5] Label Model\n",
      "model predicts 0 inconsistently.\n",
      "model predicts 0 inconsistently.\n",
      "[6] Label Model\n",
      "[7] Label Model\n",
      "[8] Label Model\n",
      "model predicts 0 inconsistently.\n",
      "model predicts 0 inconsistently.\n",
      "[9] Label Model\n",
      "[10] Label Model\n",
      "[11] Label Model\n",
      "model predicts 0 inconsistently.\n",
      "model predicts 0 inconsistently.\n",
      "[12] Label Model\n",
      "model predicts 0 inconsistently.\n",
      "model predicts 0 inconsistently.\n",
      "[13] Label Model\n",
      "model predicts 0 inconsistently.\n",
      "model predicts 0 inconsistently.\n",
      "[14] Label Model\n",
      "[15] Label Model\n",
      "model predicts 0 inconsistently.\n",
      "model predicts 0 inconsistently.\n",
      "[16] Label Model\n",
      "[17] Label Model\n",
      "[18] Label Model\n",
      "model predicts 0 inconsistently.\n",
      "model predicts 0 inconsistently.\n",
      "[19] Label Model\n",
      "model predicts 0 inconsistently.\n",
      "model predicts 0 inconsistently.\n",
      "[20] Label Model\n",
      "model predicts 0 inconsistently.\n",
      "model predicts 0 inconsistently.\n",
      "[21] Label Model\n",
      "model predicts 0 inconsistently.\n",
      "model predicts 0 inconsistently.\n",
      "[22] Label Model\n",
      "[23] Label Model\n",
      "model predicts 0 inconsistently.\n",
      "model predicts 0 inconsistently.\n",
      "[24] Label Model\n",
      "[25] Label Model\n",
      "[26] Label Model\n",
      "[27] Label Model\n",
      "model predicts 0 inconsistently.\n",
      "model predicts 0 inconsistently.\n",
      "[28] Label Model\n",
      "[29] Label Model\n",
      "[30] Label Model\n",
      "[31] Label Model\n",
      "[32] Label Model\n",
      "[33] Label Model\n"
     ]
    }
   ],
   "source": [
    "predicted_s = train_plus(train_s_candidates, test_s_ebm_candidates, test_s_ebm_corr_candidates, Y_s, 's', paramgrid = param_grid, mode = 'pred')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a260d6ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3768fb79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
