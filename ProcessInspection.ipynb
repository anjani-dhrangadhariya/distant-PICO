{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "from elasticsearch_dsl import Search,  Q\n",
    "from datetime import datetime\n",
    "import sys, json, os\n",
    "import difflib \n",
    "import uuid\n",
    "import spacy\n",
    "import ast\n",
    "from statistics import mean, median\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import traceback\n",
    "import json\n",
    "import gc\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readRawCandidates( list_NCT, label_type=None ):\n",
    "\n",
    "    nct_ids = []\n",
    "    tokens = []\n",
    "    labels = []\n",
    "    pos = []\n",
    "\n",
    "    with open(list_NCT, 'r', encoding='latin1') as NCT_ids_file:\n",
    "\n",
    "        for i, eachLine in enumerate(NCT_ids_file):\n",
    "            annot = json.loads(eachLine)\n",
    "            id_ = annot['id']\n",
    "\n",
    "            for target_key, target in annot.items():\n",
    "\n",
    "                if 'id' not in target_key:\n",
    "                    for sentence_key, sentence in target.items():\n",
    "\n",
    "                        if set(sentence['tokens'])!={0}:\n",
    "                            tokens.append( sentence['tokens'] )\n",
    "                            labels.append( sentence['annotation'] )\n",
    "                            nct_ids.append( id_ )\n",
    "\n",
    "                            # Generate dummy POS items\n",
    "                            pos_i = [0] * len( sentence['tokens'] )\n",
    "                            pos.append( pos_i )\n",
    "                        else:\n",
    "                            print('All the labels are nil')\n",
    "\n",
    "    corpus_df = pd.DataFrame(\n",
    "        {'ids': nct_ids,\n",
    "        'tokens': tokens,\n",
    "        'labels': labels,\n",
    "        'pos': pos\n",
    "        })\n",
    "\n",
    "    df = corpus_df.sample(frac=1).reset_index(drop=True) # Shuffles the dataframe after creation\n",
    "    \n",
    "    # can delete this one (corpusDf)\n",
    "    del corpus_df\n",
    "    gc.collect() # mark if for garbage collection\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority and minority labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = '/mnt/nas2/data/systematicReview/clinical_trials_gov/Weak_PICO/PICOS_data_preprocessed/merged_1_0.txt'\n",
    "\n",
    "df = readRawCandidates( merged_data, label_type=None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_collection = []\n",
    "\n",
    "for eachTokenList in df['labels']:\n",
    "    token_collection.extend( eachTokenList )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_counted = Counter(token_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_minor = ( (labels_counted[1] + labels_counted[2] + labels_counted[3] + labels_counted[4]) / labels_counted[0] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of minority labels:  10.577408759933324\n"
     ]
    }
   ],
   "source": [
    "print('The percentage of minority labels: ', percent_minor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464037\n",
      "464037\n",
      "Longest outcome term:  68\n",
      "Longest outcome term:  1\n",
      "Mean outcome term:  10.124744363057257\n",
      "Median outcome term:  8\n",
      "####################################################################\n",
      "[('NOUN', 1589426), ('PROPN', 744440), ('ADP', 718375), ('PUNCT', 510399), ('ADJ', 389293), ('VERB', 212547), ('DET', 168427), ('NUM', 144452), ('CCONJ', 118636), ('PART', 49679), ('ADV', 19831), ('PRON', 11321), ('AUX', 6121), ('X', 5819), ('SCONJ', 5440), ('SYM', 3718), ('INTJ', 332)]\n",
      "####################################################################\n",
      "[('NN', 1332428), ('IN', 722014), ('NNP', 686128), ('JJ', 380177), ('NNS', 257011), ('-RRB-', 176226), ('-LRB-', 169869), ('DT', 164240), ('CD', 144452), ('CC', 118636), ('VBN', 89007), (',', 68176), ('NNPS', 58312), ('VBG', 47613), ('.', 44736), ('TO', 42760), ('VB', 38053), (':', 31667), ('RB', 17871), ('HYPH', 15538), ('VBD', 15028), ('VBP', 13895), ('VBZ', 8951), ('WP', 8099), ('MD', 6121), ('POS', 5858), ('JJR', 5088), ('JJS', 4028), ('WDT', 3841), ('SYM', 3605), ('FW', 3495), ('WRB', 2428), ('``', 2150), ('PRP$', 2077), (\"''\", 1809), ('RP', 1801), ('XX', 1477), ('PRP', 971), ('LS', 813), ('RBR', 442), ('UH', 332), ('WP$', 250), ('NFP', 228), ('EX', 161), ('RBS', 151), ('$', 113), ('PDT', 96), ('ADD', 34)]\n",
      "####################################################################\n",
      "Total number of tokens:  4698256\n",
      "of \t 251263\n",
      ") \t 168958\n",
      "( \t 165230\n",
      "in \t 125575\n",
      "the \t 125026\n",
      "and \t 88071\n",
      ", \t 68112\n",
      "change \t 66907\n",
      "to \t 64649\n",
      "with \t 55520\n"
     ]
    }
   ],
   "source": [
    "prim_outcomes = '/mnt/nas2/data/systematicReview/clinical_trials_gov/distant_pico_pre/primary_outcomes.txt'\n",
    "#prim_outcomes = '/mnt/nas2/data/systematicReview/clinical_trials_gov/distant_pico_pre/secondary_outcomes.txt'\n",
    "counter = 0\n",
    "\n",
    "outcome_names = []\n",
    "outcome_tokens = []\n",
    "pos_all = []\n",
    "posfine_all = []\n",
    "outcome_tokens_all = []\n",
    "\n",
    "with open(prim_outcomes, 'r') as pof:\n",
    "    try:\n",
    "        \n",
    "        for eachOutcome in pof:\n",
    "            counter = counter + 1\n",
    "            #rint(counter)\n",
    "            j = json.loads(eachOutcome)\n",
    "\n",
    "            if j:\n",
    "                for key, value in j.items():\n",
    "                    for eachOne in value:\n",
    "                        if 'text' in eachOne:\n",
    "                            outcome_names.append( eachOne['text'] )\n",
    "                        if 'tokens' in eachOne:\n",
    "                            outcome_tokens.append( eachOne['tokens'] )\n",
    "                            outcome_tokens_all.extend( list(map(lambda x: x.lower(), eachOne['tokens'])) ) \n",
    "                        if 'pos' in eachOne:\n",
    "                            pos_all.extend( eachOne['pos'] )\n",
    "                        if 'pos_fine' in eachOne:\n",
    "                            posfine_all.extend( eachOne['pos_fine'] )\n",
    "\n",
    "            #f counter == 10:\n",
    "            #  break\n",
    "    except:\n",
    "        print('something strange happened')\n",
    "\n",
    "print( len(outcome_names) )\n",
    "print( len(outcome_tokens) )\n",
    "\n",
    "\n",
    "longest_outcome = max(map(len, outcome_tokens))\n",
    "print('Longest outcome term: ', longest_outcome)\n",
    "\n",
    "shortest_outcome = min(map(len, outcome_tokens))\n",
    "print('Longest outcome term: ', shortest_outcome)\n",
    "\n",
    "mean_outcome = mean(map(len, outcome_tokens))\n",
    "print('Mean outcome term: ', mean_outcome)\n",
    "\n",
    "median_outcome = median(map(len, outcome_tokens))\n",
    "print('Median outcome term: ', median_outcome)\n",
    "print('####################################################################')\n",
    "pos_counter = Counter(pos_all)\n",
    "print( pos_counter.most_common(50) )\n",
    "print('####################################################################')\n",
    "posfine_counter = Counter(posfine_all)\n",
    "print( posfine_counter.most_common(50) )\n",
    "\n",
    "print('####################################################################')\n",
    "# Most common words\n",
    "print('Total number of tokens: ', len(outcome_tokens_all))\n",
    "outcometerms_counter = Counter(outcome_tokens_all)\n",
    "most_common_outcome_terms = outcometerms_counter.most_common(10)\n",
    "\n",
    "for tuple_i in most_common_outcome_terms:\n",
    "    print(tuple_i[0], '\\t', tuple_i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF POC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ifidf_for_words(text):\n",
    "    tfidf_matrix= vectorizer.transform([text]).todense()\n",
    "    feature_index = tfidf_matrix[0,:].nonzero()[1]\n",
    "    tfidf_scores = zip([feature_names[i] for i in feature_index], [tfidf_matrix[0, x] for x in feature_index])\n",
    "    sorted_dict = {k: v for k, v in sorted(dict(tfidf_scores).items(), key=lambda item: item[1])}\n",
    "    return sorted_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_tfidf = vectorizer.fit_transform(outcome_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_string = 'Flexion knees measured goniometer side side difference expressed degrees.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'measured': 0.168605292180929,\n",
       " 'difference': 0.2244409287049211,\n",
       " 'expressed': 0.3000135047712927,\n",
       " 'flexion': 0.3118107683732015,\n",
       " 'degrees': 0.3191784079340699,\n",
       " 'goniometer': 0.38741566434403935,\n",
       " 'knees': 0.41333903188241167,\n",
       " 'side': 0.5578078148467132}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ifidf_for_words(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine overlapping spans in PICOS weak annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['p', 'ic', 'o', 's']\n",
    "label_combinations = itertools.combinations(labels, 2)\n",
    "label_combinations = list(label_combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('p', 'ic'), ('p', 'o'), ('p', 's'), ('ic', 'o'), ('ic', 's'), ('o', 's')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage overlap between  p  and  o  is:  13.003077294394497\n"
     ]
    }
   ],
   "source": [
    "l1 = label_combinations[1][0]\n",
    "l2 =  label_combinations[1][1]\n",
    "\n",
    "overlaps = []\n",
    "non_overlaps = []\n",
    "\n",
    "annotations_global = '/home/anjani/distant-PICO/CandidateGeneration/ResultInspection/pico_multiclass.txt'\n",
    "#annotations_global = '/home/anjani/distant-PICO/CandidateGeneration/ResultInspection/label_overlap_inspection.txt'\n",
    "\n",
    "counter = 0\n",
    "with open(annotations_global, 'r', encoding='latin1') as af:\n",
    "\n",
    "        for annot in af: # Each annotation file\n",
    "            try:\n",
    "                counter = counter + 1\n",
    "                j = json.loads( annot )\n",
    "                for k,v in j.items(): # target\n",
    "                    if 'id' not in k:\n",
    "                        for k_i, v_i in v.items(): # each sentence\n",
    "                            if l1 in v_i and l2 in v_i:\n",
    "                                phrase = []\n",
    "                                non_o_phrase = []\n",
    "                                list1 = v_i[l1]\n",
    "                                list2 = v_i[l2]\n",
    "                                for n, (a1, a2) in enumerate( zip(list1, list2) ):\n",
    "                                    if a1 != 0 and a2 != 0:\n",
    "                                        phrase.append( v_i['tokens'][n] )\n",
    "                                    if a1 != 0 or a2 != 0:\n",
    "                                        non_o_phrase.append( v_i['tokens'][n] )\n",
    "                                if phrase:\n",
    "                                    overlaps.append( ' '.join(phrase) )\n",
    "                                if non_o_phrase:\n",
    "                                    non_overlaps.append( ' '.join(non_o_phrase) )\n",
    "            except Exception as ex:\n",
    "                pass\n",
    "                template = \"An exception of type {0} occurred. Arguments:{1!r}\"\n",
    "                message = template.format(type(ex).__name__, ex.args)\n",
    "                #print( message )\n",
    "\n",
    "                exc_type, exc_obj, exc_tb = sys.exc_info()\n",
    "                fname = os.path.split(exc_tb.tb_frame.f_code.co_filename)[1]\n",
    "                #print(exc_type, fname, exc_tb.tb_lineno)\n",
    "\n",
    "                #print(traceback.format_exc())\n",
    "\n",
    "print('Percentage overlap between ', l1, ' and ', l2, ' is: ', (len( set( overlaps ) ) / len( list(non_overlaps) ) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore \"outcome\" sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of non-null outcome sources retrieved 473893\n"
     ]
    }
   ],
   "source": [
    "outcomes_file = '/mnt/nas2/data/systematicReview/clinical_trials_gov/distant_pico_pre/temp.txt'\n",
    "\n",
    "outcomes = dict()\n",
    "\n",
    "counter = 0\n",
    "with open(outcomes_file, 'r') as rf:\n",
    "    for eachOutcome in rf:\n",
    "        j = json.loads( eachOutcome )\n",
    "        if j:\n",
    "            counter = counter + 1\n",
    "            outcomes[counter] = j\n",
    "            \n",
    "            \n",
    "print('The number of non-null outcome sources retrieved', len( outcomes ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PrimaryOutcome_0': [{'text': '( 0 - 10) Itching scale',\n",
       "   'tokens': ['(', '0', '-', '10', ')', 'Itching', 'scale'],\n",
       "   'lemma': ['(', '0', '-', '10', ')', 'itching', 'scale'],\n",
       "   'pos': ['PUNCT', 'NUM', 'PUNCT', 'NUM', 'PUNCT', 'NOUN', 'NOUN'],\n",
       "   'pos_fine': ['-LRB-', 'CD', 'HYPH', 'CD', '-RRB-', 'NN', 'NN']}]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcomes[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "tokens = []\n",
    "pos = []\n",
    "pos_fine = []\n",
    "tokens_lengths = []\n",
    "\n",
    "counter_i = 0\n",
    "for key, value in outcomes.items():\n",
    "    for a_key, a_value in value.items():\n",
    "        text.append( a_value[0]['text'] )\n",
    "        tokens.append( a_value[0]['tokens'] )\n",
    "        pos.append( a_value[0]['pos'] )\n",
    "        pos_fine.append( a_value[0]['pos_fine'] )\n",
    "        tokens_lengths.append( len(a_value[0]['tokens']) )\n",
    "        counter_i = counter_i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'text' : text, 'tokens' : tokens, 'pos' : pos, 'pos_fine' : pos_fine })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>pos</th>\n",
       "      <th>pos_fine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1564619</th>\n",
       "      <td>Oswestry Disability Index</td>\n",
       "      <td>[Oswestry, Disability, Index]</td>\n",
       "      <td>[PROPN, PROPN, PROPN]</td>\n",
       "      <td>[NNP, NNP, NNP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564620</th>\n",
       "      <td>Numeric Rating Scale</td>\n",
       "      <td>[Numeric, Rating, Scale]</td>\n",
       "      <td>[PROPN, PROPN, PROPN]</td>\n",
       "      <td>[NNP, NNP, NNP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564621</th>\n",
       "      <td>Medical Outcomes Study Short-Form Health Surve...</td>\n",
       "      <td>[Medical, Outcomes, Study, Short-Form, Health,...</td>\n",
       "      <td>[PROPN, PROPN, PROPN, PROPN, PROPN, PROPN, NOU...</td>\n",
       "      <td>[NNP, NNP, NNP, NNP, NNP, NNP, NN, CD, -LRB-, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564622</th>\n",
       "      <td>Centre for Epidemiological Studies Depression ...</td>\n",
       "      <td>[Centre, for, Epidemiological, Studies, Depres...</td>\n",
       "      <td>[NOUN, ADP, PROPN, PROPN, PROPN, PROPN]</td>\n",
       "      <td>[NN, IN, NNP, NNPS, NNP, NNP]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564623</th>\n",
       "      <td>Zurich claudification Scale, Numeric Rating Sc...</td>\n",
       "      <td>[Zurich, claudification, Scale, ,, Numeric, Ra...</td>\n",
       "      <td>[ADJ, NOUN, PROPN, PUNCT, PROPN, PROPN, PROPN,...</td>\n",
       "      <td>[JJ, NN, NNP, ,, NNP, NNP, NNP, IN, NN, NN, ,,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  \\\n",
       "1564619                          Oswestry Disability Index   \n",
       "1564620                               Numeric Rating Scale   \n",
       "1564621  Medical Outcomes Study Short-Form Health Surve...   \n",
       "1564622  Centre for Epidemiological Studies Depression ...   \n",
       "1564623  Zurich claudification Scale, Numeric Rating Sc...   \n",
       "\n",
       "                                                    tokens  \\\n",
       "1564619                      [Oswestry, Disability, Index]   \n",
       "1564620                           [Numeric, Rating, Scale]   \n",
       "1564621  [Medical, Outcomes, Study, Short-Form, Health,...   \n",
       "1564622  [Centre, for, Epidemiological, Studies, Depres...   \n",
       "1564623  [Zurich, claudification, Scale, ,, Numeric, Ra...   \n",
       "\n",
       "                                                       pos  \\\n",
       "1564619                              [PROPN, PROPN, PROPN]   \n",
       "1564620                              [PROPN, PROPN, PROPN]   \n",
       "1564621  [PROPN, PROPN, PROPN, PROPN, PROPN, PROPN, NOU...   \n",
       "1564622            [NOUN, ADP, PROPN, PROPN, PROPN, PROPN]   \n",
       "1564623  [ADJ, NOUN, PROPN, PUNCT, PROPN, PROPN, PROPN,...   \n",
       "\n",
       "                                                  pos_fine  \n",
       "1564619                                    [NNP, NNP, NNP]  \n",
       "1564620                                    [NNP, NNP, NNP]  \n",
       "1564621  [NNP, NNP, NNP, NNP, NNP, NNP, NN, CD, -LRB-, ...  \n",
       "1564622                      [NN, IN, NNP, NNPS, NNP, NNP]  \n",
       "1564623  [JJ, NN, NNP, ,, NNP, NNP, NNP, IN, NN, NN, ,,...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_percents = dict()\n",
    "\n",
    "for i in range(min(tokens_lengths), max(tokens_lengths)):\n",
    "    #print('Percentage of the outcome mentions with token length ', str(i) , ' : ', ( tokens_lengths.count( i ) / len(tokens_lengths) ) * 100 )\n",
    "    token_percents[i] = ( tokens_lengths.count( i ) / len(tokens_lengths) ) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
